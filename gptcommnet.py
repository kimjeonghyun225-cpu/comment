# -*- coding: utf-8 -*-
# comment1.py â€” ìµœì¢…ë³¸
# ëª©í‘œ: ì½”ë©˜íŠ¸ í’ˆì§ˆ ìµœìš°ì„  / êµ°ì§‘ ë‹¨ìœ„ GPT ë¯¸ë‹ˆ í”„ë¡¬í”„íŠ¸ / qa_patch_module ì˜ì¡´ ì œê±°

import os
import re
import io
import time
import zipfile
import unicodedata
from contextlib import contextmanager
from collections import defaultdict

import pandas as pd
import openpyxl
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

# =========================
# í™˜ê²½ì„¤ì •
# =========================
load_dotenv()
api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))
if not api_key:
    st.error("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. st.secrets ë˜ëŠ” .envì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    st.stop()
client = OpenAI(api_key=api_key)

st.set_page_config(page_title="QA ê²°ê³¼ ìë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°", layout="wide")
st.title(":bar_chart: QA ê²°ê³¼ ìë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°")

# ì„¸ì…˜ ì´ˆê¸°í™” ë²„íŠ¼(í”„ë¡œì íŠ¸ ê°„ ì”ì—¬ ìƒíƒœ ì œê±°)
if st.button("ğŸ”„ ì„¸ì…˜ ì´ˆê¸°í™”(ëª¨ë“  ë‚´ë¶€ ìƒíƒœ ë¦¬ì…‹)"):
    st.session_state.clear()
    st.rerun()

# =========================
# ê³µí†µ ìœ í‹¸
# =========================
@contextmanager
def step_status(title: str):
    with st.status(title, expanded=False) as status:
        t0 = time.time()
        try:
            yield status
            status.update(label=f"{title} - ì™„ë£Œ ({time.time()-t0:.2f}s)", state="complete", expanded=False)
        except Exception as e:
            status.update(label=f"{title} - ì‹¤íŒ¨: {e}", state="error", expanded=True)
            raise

def diag_dump(label: str, obj):
    with st.expander(f"ğŸ” ì§„ë‹¨ ë³´ê¸°: {label}", expanded=False):
        st.write(obj)

def _norm(s: str) -> str:
    s = unicodedata.normalize("NFKC", str(s))
    s = re.sub(r"[\s\-\_/()\[\]{}:+Â·âˆ™â€¢]", "", s)
    return s.lower().strip()

def normalize_model_name_strict(s):
    if pd.isna(s): return ""
    s = str(s)
    s = re.sub(r"\(.*?\)", "", s)
    s = re.sub(r"\b(64|128|256|512)\s*gb\b", "", s, flags=re.I)
    s = re.sub(r"\b(black|white|blue|red|green|gold|silver|ê³¨ë“œ|ë¸”ë™|í™”ì´íŠ¸|ì‹¤ë²„)\b", "", s, flags=re.I)
    s = re.sub(r"[\s\-_]+", "", s)
    return s.lower().strip()

# =========================
# í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ìë™ ê°ì§€
# =========================
def find_test_sheet_candidates(xls) -> list:
    names = [str(n) for n in getattr(xls, "sheet_names", [])]
    patterns = [
        r"(?i)\btest\s*case\b.*\b(aos|android)\b",
        r"(?i)\btest\s*case\b.*\b(ios)\b",
        r"(?i)\btestcase(?:[ _\-]*)aos\b",
        r"(?i)\btestcase(?:[ _\-]*)ios\b",
        r"(?i)\bcompatibility\s*test\b.*\b(aos|android)\b",
        r"(?i)\bcompatibility\s*test\b.*\b(ios)\b",
        r"(?i)í˜¸í™˜ì„±\s*í…ŒìŠ¤íŠ¸.*(aos|android|ios)",
        r"(?i)compatibility\s*test\((?:aos|ios)\)",
        r"(?i)\btc[_\- ]?(aos|android)\b",
        r"(?i)\btc[_\- ]?ios\b",
        r"(?i)\bcompat[_\- ]?test[_\- ]?[a-z]?\b",
    ]
    cands = set()
    for n in names:
        for p in patterns:
            try:
                if re.search(p, n):
                    cands.add(n); break
            except re.error:
                continue
    if not cands:
        for n in names:
            norm = re.sub(r"[\s_\-]+", "", n.lower())
            if any(k in norm for k in ["testcase","compatibilitytest","í…ŒìŠ¤íŠ¸","í˜¸í™˜ì„±","tc_","tc-","tc "]):
                cands.add(n)
    return sorted(cands) if cands else names

# =========================
# Fail + ì…€ ì½”ë©˜íŠ¸ ì¶”ì¶œ + ë¹„ê³ /Notes ë³‘í•©
# =========================
def find_row_by_labels(ws, labels, search_rows=30, search_cols=70):
    max_r = min(search_rows, ws.max_row)
    max_c = min(search_cols, ws.max_column)
    target = set(str(x).strip() for x in labels)
    for r in range(1, max_r + 1):
        for c in range(1, max_c + 1):
            v = ws.cell(row=r, column=c).value
            if v and str(v).strip() in target:
                return r
    return 0

def get_checklist_label(ws, row):
    label_parts, columns_to_check = [], [6, 7, 9]
    for c in columns_to_check:
        for r_search in range(row, 0, -1):
            cell_value = ws.cell(row=r_search, column=c).value
            if cell_value and str(cell_value).strip():
                label_parts.append(str(cell_value).replace("\n", " ").strip())
                break
    return " / ".join(label_parts)

def extract_comments_as_dataframe(wb, target_sheet_names):
    extracted = []
    for sheet_name in target_sheet_names:
        if sheet_name not in wb.sheetnames:
            continue
        ws = wb[sheet_name]
        header_rows = {
            "Model":   find_row_by_labels(ws, ["Model", "ì œí’ˆëª…"]),
            "Chipset": find_row_by_labels(ws, ["Chipset", "CPU", "AP"]),
            "RAM":     find_row_by_labels(ws, ["RAM", "ë©”ëª¨ë¦¬"]),
            "Rank":    find_row_by_labels(ws, ["Rating Grade?", "Rank", "ë“±ê¸‰"]),
            "OS":      find_row_by_labels(ws, ["OS Version", "Android", "iOS", "OS"]),
        }
        for row in ws.iter_rows():
            for cell in row:
                val = cell.value
                if isinstance(val, str) and val.strip().lower() == "fail" and cell.comment:
                    r, c = cell.row, cell.column
                    device_info = {
                        key: ws.cell(row=num, column=c).value if num > 0 else ""
                        for key, num in header_rows.items()
                    }
                    checklist = get_checklist_label(ws, r)
                    comment_text = (cell.comment.text or "").split(
                        "https://go.microsoft.com/fwlink/?linkid=870924.", 1
                    )[-1].strip()
                    extracted.append({
                        "Sheet": ws.title,
                        "Device(Model)": device_info.get("Model", ""),
                        "Chipset": device_info.get("Chipset", ""),
                        "RAM": device_info.get("RAM", ""),
                        "Rank": device_info.get("Rank", ""),
                        "OS": device_info.get("OS", ""),
                        "Checklist": checklist,
                        "comment_cell": comment_text,
                        "Comment(Text)": "",
                    })
    if not extracted:
        return None
    return pd.DataFrame(extracted)

def _nkfc(s: str) -> str:
    if pd.isna(s): return ""
    s = str(s)
    s = re.sub(r"\s+", "", s)
    s = re.sub(r"[_\-\/(){}\[\]:+Â·âˆ™â€¢]", "", s)
    return s.strip().lower()

def _safe_series(df: pd.DataFrame, col: str) -> pd.Series:
    return df[col] if col in df.columns else pd.Series([""] * len(df), index=df.index)

def _pick_first_nonempty(*series):
    out = pd.Series([""] * len(series[0]), index=series[0].index, dtype="object")
    for s in series:
        s2 = s.fillna("").astype(str)
        mask = (out == "") & (s2.str.len() > 0)
        out.loc[mask] = s2.loc[mask]
    return out

def enrich_with_column_comments(xls, test_sheet_name: str, df_issues: pd.DataFrame) -> pd.DataFrame:
    issues = df_issues.copy()
    base_comment = _pick_first_nonempty(
        _safe_series(issues, "comment_cell"),
        _safe_series(issues, "Comment(Text)"),
    )
    issues["comment_text"] = base_comment.fillna("").astype(str)
    if "evidence_links" not in issues.columns:
        issues["evidence_links"] = [[] for _ in range(len(issues))]

    try:
        df_tbl = pd.read_excel(xls, sheet_name=test_sheet_name, engine="openpyxl")
    except Exception:
        return issues
    if df_tbl is None or df_tbl.empty:
        return issues

    note_candidates = [
        c for c in df_tbl.columns
        if str(c).strip().lower() in {"notes","note","ë¹„ê³ ","comment","comments","ì½”ë©˜íŠ¸"}
    ]
    if not note_candidates:
        return issues

    issues_keys = []
    for k in ["Checklist", "Device(Model)"]:
        if (k in issues.columns) and (k in df_tbl.columns):
            issues[f"__key_{k}__"] = issues[k].map(_nkfc)
            df_tbl[f"__key_{k}__"]  = df_tbl[k].map(_nkfc)
            issues_keys.append(f"__key_{k}__")
    if not issues_keys:
        return issues

    note_df = df_tbl[issues_keys + note_candidates].copy()
    for c in note_candidates:
        note_df[c] = note_df[c].astype(str).fillna("")
    agg_map = {c: lambda s: " / ".join([x for x in s if x and x.lower() != "nan"]) for c in note_candidates}
    note_df = note_df.groupby(issues_keys, as_index=False).agg(agg_map)

    merged = pd.merge(issues, note_df, how="left", left_on=issues_keys, right_on=issues_keys)

    def _row_notes(row):
        vals = []
        for c in note_candidates:
            v = row.get(c, "")
            if isinstance(v, str) and v and v.lower() != "nan":
                vals.append(v.strip())
        return " | ".join(vals)

    notes_joined = merged.apply(_row_notes, axis=1)
    merged["comment_text"] = (
        merged["comment_text"].astype(str).str.strip()
        + ((" | " + notes_joined).where(notes_joined.str.len() > 0, ""))
    ).str.strip(" |")

    if "evidence_links" not in merged.columns:
        merged["evidence_links"] = [[] for _ in range(len(merged))]

    return merged

# =========================
# ìŠ¤í™ ì‹œíŠ¸ ë³‘í•©
# =========================
def _norm_for_header(s: str) -> str:
    s = unicodedata.normalize("NFKC", str(s))
    s = re.sub(r"[\s\-\_/()\[\]{}:+Â·âˆ™â€¢]", "", s)
    return s.lower().strip()

def load_std_spec_df(xls, sheet):
    df_probe = pd.read_excel(xls, sheet_name=sheet, header=None, engine="openpyxl")
    header_row_idx = 0
    header_candidates = [r"^model$", r"^device$", r"^ì œí’ˆëª…$", r"^ì œí’ˆ$", r"^ëª¨ë¸ëª…$", r"^ëª¨ë¸$"]
    for r in range(min(12, len(df_probe))):
        row_vals = df_probe.iloc[r].astype(str).fillna("")
        norm_vals = [_norm_for_header(v) for v in row_vals]
        for v in norm_vals:
            if any(re.search(pat, v) for pat in header_candidates):
                header_row_idx = r; break
        if header_row_idx:
            break

    df = pd.read_excel(xls, sheet_name=sheet, header=header_row_idx, engine="openpyxl")
    original_cols = list(df.columns)
    norm_cols = [_norm_for_header(c) for c in original_cols]
    synonyms = {
        r"^(model|device|ì œí’ˆëª…|ì œí’ˆ|ëª¨ë¸ëª…|ëª¨ë¸)$": "Model",
        r"^(maker|manufacturer|brand|oem|ì œì¡°ì‚¬|ë²¤ë”)$": "ì œì¡°ì‚¬",
        r"^(gpu|ê·¸ë˜í”½|ê·¸ë˜í”½ì¹©|ê·¸ë˜í”½ìŠ¤|ê·¸ë˜í”½í”„ë¡œì„¸ì„œ)$": "GPU",
        r"^(chipset|soc|ap|cpu)$": "Chipset",
        r"^(ram|ë©”ëª¨ë¦¬)$": "RAM",
        r"^(os|osversion|android|ios|íŒì›¨ì–´|ì†Œí”„íŠ¸ì›¨ì–´ë²„ì „)$": "OS",
        r"^(rank|rating|ratinggrade|ë“±ê¸‰)$": "Rank",
    }
    col_map = {}
    for norm_name, orig_name in zip(norm_cols, original_cols):
        mapped = None
        for pat, std_name in synonyms.items():
            if re.search(pat, norm_name):
                mapped = std_name; break
        col_map[orig_name] = mapped or orig_name
    df = df.rename(columns=col_map)

    def _normalize_model(s):
        if pd.isna(s): return ""
        s = str(s)
        s = re.sub(r"\(.*?\)", "", s)
        s = re.sub(r"[\s\-_]+", "", s)
        return s.lower().strip()

    model_col = "Model" if "Model" in df.columns else None
    if model_col is None:
        for c in df.columns:
            if re.search(r"^(model|device|ì œí’ˆëª…|ì œí’ˆ|ëª¨ë¸ëª…|ëª¨ë¸)$", _norm_for_header(c)):
                model_col = c; break
    if model_col is None:
        raise ValueError(f"'{sheet}'ì—ì„œ ëª¨ë¸ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {list(df.columns)}")

    df["model_norm"] = df[model_col].apply(_normalize_model)
    cols_keep = ["model_norm"]
    for c in ["GPU","ì œì¡°ì‚¬","Chipset","RAM","OS","Rank","Model"]:
        if c in df.columns:
            cols_keep.append(c)
    return df[cols_keep]

# =========================
# ë¡œê·¸ ìš”ì•½(ì„ íƒ) & ê·¼ë³¸ì›ì¸ ì¶”ì •
# =========================
def load_and_summarize_logcat_files(files):
    patterns = {
        "crash": re.compile(r"\bFATAL EXCEPTION\b|\bAbort message:\b|\bbacktrace\b", re.I),
        "anr": re.compile(r"\bANR in\b|\bApplication Not Responding\b", re.I),
        "gl_err": re.compile(r"(E/libEGL|E/GLConsumer|OpenGLRenderer|Adreno|Mali)", re.I),
        "thermal": re.compile(r"(thermal|ThermalEngine|throttl)", re.I),
        "net": re.compile(r"(SocketTimeout|UnknownHost|SSLHandshake|Network is unreachable)", re.I),
        "fps": re.compile(r"\bFPS[:=]\s*\d+", re.I),
    }
    total_counts = {k: 0 for k in patterns.keys()}
    file_count = 0

    def _consume_text(txt: str):
        for k, p in patterns.items():
            total_counts[k] += len(p.findall(txt))

    for f in files:
        name = f.name.lower()
        try:
            if name.endswith(".zip"):
                with zipfile.ZipFile(io.BytesIO(f.read())) as zf:
                    for info in zf.infolist():
                        if info.is_dir(): continue
                        if not info.filename.lower().endswith((".txt", ".log")): continue
                        with zf.open(info) as zfh:
                            data = zfh.read()
                            txt = data.decode("utf-8", errors="ignore")
                            _consume_text(txt)
                            file_count += 1
            else:
                data = f.read()
                txt = data.decode("utf-8", errors="ignore")
                _consume_text(txt)
                file_count += 1
        except Exception:
            continue

    parts = [f"{k}:{v}" for k, v in total_counts.items()]
    return {"log_summary": f"files={file_count}; " + ", ".join(parts)}

def _parse_log_summary(summary: str) -> dict:
    out = {"files": 0, "crash": 0, "anr": 0, "gl_err": 0, "thermal": 0, "net": 0, "fps": 0}
    if not summary:
        return out
    try:
        parts = [p.strip() for p in summary.split(";")]
        if parts and parts[0].startswith("files="):
            out["files"] = int(parts[0].split("=",1)[1])
        tail = parts[1] if len(parts) > 1 else ""
        for kv in tail.split(","):
            if ":" in kv:
                k, v = kv.split(":", 1)
                if k.strip() in out:
                    out[k.strip()] = int(v.strip())
    except Exception:
        pass
    return out

def infer_root_causes_from_logs(summary: str) -> list:
    c = _parse_log_summary(summary)
    hyps = []
    if c.get("gl_err", 0) >= 3 or (c.get("fps", 0) >= 10 and c.get("gl_err", 0) >= 1):
        hyps.append({"signal": "gl_err/fps", "hypothesis": "GPU ë“œë¼ì´ë²„/ë Œë”ë§ ë³‘ëª© ê°€ëŠ¥", "evidence": f"gl_err={c.get('gl_err',0)}, fps={c.get('fps',0)}"})
    if c.get("crash", 0) >= 2:
        hyps.append({"signal": "crash", "hypothesis": "ë„¤ì´í‹°ë¸Œ í¬ë˜ì‹œ(ë©”ëª¨ë¦¬/ë„í¬ì¸í„°) ê°€ëŠ¥", "evidence": f"crash={c.get('crash',0)}"})
    if c.get("anr", 0) >= 1:
        hyps.append({"signal": "anr", "hypothesis": "ë©”ì¸ìŠ¤ë ˆë“œ ë¸”ë¡œí‚¹/IO ì§€ì—°", "evidence": f"anr={c.get('anr',0)}"})
    if c.get("thermal", 0) >= 1:
        hyps.append({"signal": "thermal", "hypothesis": "ì¨ë©€ ìŠ¤ë¡œí‹€ë§ìœ¼ë¡œ ì¸í•œ í´ëŸ­ ì €í•˜", "evidence": f"thermal={c.get('thermal',0)}"})
    if c.get("net", 0) >= 2:
        hyps.append({"signal": "net", "hypothesis": "ë„¤íŠ¸ì›Œí¬ ì§€ì—°/SSL ì˜¤ë¥˜", "evidence": f"net={c.get('net',0)}"})
    return hyps

# =========================
# í† í° ì ˆì•½: ì½”ë©˜íŠ¸ ì••ì¶•/íƒœê·¸ ì¶”ì¶œ
# =========================
def compact_text(s: str, max_len=180):
    if not isinstance(s, str): return ""
    s = re.sub(r"\s+", " ", s).strip()
    return s[:max_len]

def digest_comments(series, topn=10, max_len=180):
    vc = (series.astype(str)
          .map(lambda x: re.sub(r"\s+", " ", x or "").strip())
          .replace({"nan": ""})
          .value_counts())
    keys = [compact_text(k, max_len) for k in vc.index.tolist() if k][:topn]
    return keys

def extract_issue_tags(text: str) -> list:
    if not isinstance(text, str): return []
    t = text.lower()
    tags = set()
    # UI/ë””ìŠ¤í”Œë ˆì´/ë ˆì´ì•„ì›ƒ
    if re.search(r"(punch[\s\-]?hole|í€ì¹˜í™€)", t): tags.add("punch_hole")
    if re.search(r"(notch|ë…¸ì¹˜)", t): tags.add("notch")
    if re.search(r"(fold|í´ë”ë¸”|í”Œë ‰ìŠ¤)", t): tags.add("foldable")
    if re.search(r"(rotation|íšŒì „|landscape|portrait)", t): tags.add("rotation")
    if re.search(r"(resolution|í•´ìƒë„|dpi|density|í…ìŠ¤ì²˜|blur|íë¦¿|ê¹¨ì§|ì•„ì´ì½˜|í°íŠ¸)", t): tags.add("ui_render")
    # ì„±ëŠ¥/ê·¸ë˜í”½/ë°œì—´
    if re.search(r"(fps|frame|stutter|ëŠê¹€)", t): tags.add("fps_drop")
    if re.search(r"(thermal|ì¨ë©€|throttl|ë°œì—´)", t): tags.add("thermal")
    if re.search(r"(opengl|vulkan|egl|renderer|shader|texture)", t): tags.add("gpu_render")
    # ì•ˆì •ì„±/ë„¤íŠ¸ì›Œí¬/ì…ë ¥
    if re.search(r"(crash|fatal|ì˜ˆì™¸|í¬ë˜ì‹œ)", t): tags.add("crash")
    if re.search(r"(anr|ì‘ë‹µì—†ìŒ)", t): tags.add("anr")
    if re.search(r"(ssl|handshake|unknownhost|timeout|ë„¤íŠ¸ì›Œí¬)", t): tags.add("network")
    if re.search(r"(input|í„°ì¹˜|ë°˜ì‘|ë”œë ˆì´|ì§€ì—°)", t): tags.add("input_delay")
    return sorted(tags)

def normalize_gpu(g):
    s = str(g or "").strip()
    if not s: return ""
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"\bPower\s*VR\b", "PowerVR", s, flags=re.I)
    s = re.sub(r"\bIMG\s+GE", "PowerVR GE", s, flags=re.I)
    s = re.sub(r"\bGE(\d+)\b", r"PowerVR GE\1", s, flags=re.I)
    return s

def normalize_chipset(c):
    s = str(c or "").strip()
    s = re.sub(r"\s+", " ", s)
    s = re.sub(r"\bMTK\b", "MediaTek", s, flags=re.I)
    return s

# =========================
# êµ°ì§‘ ë‹¤ì´ì œìŠ¤íŠ¸(ìŠ¤í™ì¶• + íƒœê·¸ì¶•)
# =========================
def make_cluster_digests(df_final: pd.DataFrame,
                         min_group=2,
                         per_cluster_max_samples=8) -> list:
    digests = []

    if "comment_text" not in df_final.columns:
        df_final["comment_text"] = ""

    if "issue_tags" not in df_final.columns:
        df_final["issue_tags"] = df_final["comment_text"].map(extract_issue_tags)

    def _devices(rows, k=6):
        return (rows["Device(Model)"].astype(str)
                .replace("nan","").str.strip()
                .value_counts().head(k).index.tolist())

    # ìŠ¤í™ ì¶•
    for col in ["GPU", "Chipset", "OS"]:
        if col not in df_final.columns: continue
        grp = df_final.groupby(df_final[col].astype(str).str.strip())
        for key, rows in grp:
            keyn = (key or "").strip()
            if not keyn or keyn.lower() in ["nan", "(ë¯¸ê¸°ì¬)"]: continue
            if len(rows) < min_group: continue
            d = {
                "axis": col,
                "value": keyn,
                "size": int(len(rows)),
                "devices": _devices(rows),
                "evidence_comments": digest_comments(rows["comment_text"], topn=per_cluster_max_samples),
                "example_rows": rows.head(3)[["Device(Model)","comment_text"]].to_dict(orient="records")
            }
            digests.append(d)

    # íƒœê·¸ ì¶•
    bucket = defaultdict(list)
    for i, tags in enumerate(df_final["issue_tags"]):
        for t in (tags or []):
            bucket[t].append(i)
    for t, idxs in bucket.items():
        if len(idxs) < min_group: continue
        rows = df_final.iloc[idxs]
        d = {
            "axis": "issue_tag",
            "value": t,
            "size": int(len(rows)),
            "devices": _devices(rows),
            "evidence_comments": digest_comments(rows["comment_text"], topn=per_cluster_max_samples),
            "example_rows": rows.head(3)[["Device(Model)","comment_text"]].to_dict(orient="records")
        }
        digests.append(d)

    digests.sort(key=lambda x: x["size"], reverse=True)
    return digests

# =========================
# GPT: êµ°ì§‘ ë‹¨ìœ„ ë¯¸ë‹ˆ í”„ë¡¬í”„íŠ¸
# =========================
def call_openai_cluster(client, payload: dict, max_retries=4):
    system = (
        "ë‹¹ì‹ ì€ ëª¨ë°”ì¼/ê²Œì„ QA ìˆ˜ì„ì…ë‹ˆë‹¤. ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”. "
        "ì‚¬ì‹¤ ê¸°ë°˜ ì¡°ì‚¬í˜• í‘œí˜„(í™•ì¸/ë¶„ì„/ì¶”ì •ë©ë‹ˆë‹¤)ì„ ì‚¬ìš©í•˜ê³ , "
        "ë°˜ë“œì‹œ JSON ê°ì²´ 1ê°œë§Œ ì¶œë ¥í•˜ì„¸ìš”."
    )
    user = (
        "ì•„ë˜ êµ°ì§‘ì— ëŒ€í•´ 'í˜„ìƒ/ë°œìƒê¸°ê¸°/ì˜í–¥/ì›ì¸ì¶”ì •/ê¶Œê³ 'ë¥¼ ì‘ì„±í•´ JSONìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.\n"
        "ìŠ¤í‚¤ë§ˆ: {"
        "\"title\": str, "
        "\"symptom\": str, "
        "\"evidence\": [str], "
        "\"impact\": str, "
        "\"cause\": str, "
        "\"recommendation\": str, "
        "\"priority\": \"P0|P1|P2\"}\n\n"
        + pd.io.json.dumps(payload, force_ascii=False)
    )
    last_err = None
    for i in range(max_retries):
        try:
            resp = client.chat.completions.create(
                model="gpt-4o",
                temperature=0.1,
                top_p=0.9,
                max_tokens=500,
                response_format={"type": "json_object"},
                messages=[{"role":"system","content":system},
                          {"role":"user","content":user}],
            )
            txt = resp.choices[0].message.content.strip()
            try:
                return pd.io.json.loads(txt)
            except Exception:
                first, last = txt.find("{"), txt.rfind("}")
                return pd.io.json.loads(txt[first:last+1])
        except Exception as e:
            last_err = e
            time.sleep(min(2**i + i*0.5, 12))
    raise last_err

def write_issue_with_gpt(client, digest: dict, log_hypotheses: list = None):
    payload = {
        "cluster": digest,
        "log_hypotheses": log_hypotheses or []
    }
    return call_openai_cluster(client, payload)

# =========================
# Summary í…ìŠ¤íŠ¸ ìƒì„±(í˜„ìƒ/ê¸°ê¸°/ì˜í–¥/ì›ì¸/ê¶Œê³ )
# =========================
def build_summary_block(issues: list, topn: int = 100) -> str:
    lines = []
    for i, iss in enumerate(issues or [], start=1):
        if i > topn: break
        title  = iss.get("title","ì´ìŠˆ")
        symp   = iss.get("symptom","")
        impact = iss.get("impact","")
        cause  = iss.get("cause","(ì¶”ì • ê·¼ê±° ë¶€ì¡±)")
        rec    = iss.get("recommendation","")
        dev_line = ""
        evs = iss.get("evidence") or []
        # evidence ì•ˆì— ëª¨ë¸ëª…ì´ ë“¤ì–´ìˆëŠ” ê²½ìš° ìš°ì„  ë…¸ì¶œ
        for e in evs:
            if any(k in str(e) for k in ["Galaxy","Xiaomi","iPhone","OPPO","VIVO","Redmi","Pixel","SM-"]):
                dev_line = str(e); break
        if not dev_line and evs:
            dev_line = str(evs[0])
        block = (
            f"{title}\n"
            f"* í˜„ìƒ: {symp}\n"
            f"* ë°œìƒ ê¸°ê¸°: {dev_line}\n"
            f"* ì˜í–¥: {impact}\n"
            f"* ì›ì¸ ì¶”ì •: {cause}\n"
            f"* ê¶Œê³ : {rec}"
        )
        lines.append(block.strip())
    return ("\n\n---\n\n".join(lines)).strip()

# =========================
# ë¦¬í¬íŠ¸ ì‘ì„±
# =========================
def write_excel_report(result: dict, df_final: pd.DataFrame, path: str) -> None:
    try:
        import xlsxwriter  # noqa
        engine = "xlsxwriter"
    except Exception:
        try:
            import openpyxl  # noqa
            engine = "openpyxl"
        except Exception:
            raise RuntimeError("ì—‘ì…€ ì‘ì„± ì—”ì§„ì´ ì—†ìŠµë‹ˆë‹¤. `pip install xlsxwriter` ë˜ëŠ” `pip install openpyxl`")

    with pd.ExcelWriter(path, engine=engine) as wr:
        # Executive_Summary (A/C/E)
        exec_rows = [{
            "A. í•œ ì¤„ ì´í‰": result.get("summary",""),
            "C. ë””ë°”ì´ìŠ¤ ë¦¬ìŠ¤í¬": " / ".join([d.get("device_model_or_combo","") for d in (result.get("device_risks") or [])][:5]),
            "E. ë¦´ë¦¬ìŠ¤ ê¶Œê³ ": f"{result.get('release_decision','')} / ì¡°ê±´: {result.get('conditions','')}"
        }]
        pd.DataFrame(exec_rows).to_excel(wr, sheet_name="Executive_Summary", index=False)

        # Summary â€” ìƒì„¸ ë¸”ë¡
        summary_text = build_summary_block(result.get("issues", []), topn=100)
        if not summary_text:
            summary_text = (
                f"ë¦´ë¦¬ìŠ¤ ê¶Œê³ : {result.get('release_decision','')} / ì¡°ê±´: {result.get('conditions','')}\n"
                f"- ì£¼ìš” íŒ¨í„´/êµ°ì§‘/í•µì‹¬ ë¬¸ì œ/ìš°ì„ ìˆœìœ„/ì¢…í•© ì˜ê²¬ì€ Issues ë° Device_Risksë¥¼ ì°¸ì¡°í•˜ì‹­ì‹œì˜¤."
            )
        pd.DataFrame([{"Summary & Insight": summary_text}]).to_excel(wr, sheet_name="Summary", index=False)

        # Issues â€” ì œí•œ ì—†ìŒ
        issues = pd.DataFrame(result.get("issues", []))
        if issues.empty:
            pd.DataFrame([{"title":"(ì—†ìŒ)"}]).to_excel(wr, sheet_name="Issues", index=False)
        else:
            issues.to_excel(wr, sheet_name="Issues", index=False)

        # Device_Risks
        risks = pd.DataFrame(result.get("device_risks", []))
        risks.to_excel(wr, sheet_name="Device_Risks", index=False)

        # Evidence_Sample(ì›ë³¸ ì¼ë¶€)
        cols = [c for c in ["Sheet","Device(Model)","GPU","Chipset","RAM","OS","Rank","Checklist","comment_text"] if c in df_final.columns]
        if cols:
            disp = df_final[cols].head(200).copy()
            for c in disp.columns:
                if str(disp[c].dtype) == "object" or str(disp[c].dtype).startswith("category"):
                    disp[c] = disp[c].astype(str)
            disp.to_excel(wr, sheet_name="Evidence_Sample", index=False)
        else:
            pd.DataFrame().to_excel(wr, sheet_name="Evidence_Sample", index=False)

        # Cluster_* (ì„ íƒ)
        metrics_in_result = result.get("metrics", {})
        clusters = metrics_in_result.get("clusters", {}) if isinstance(metrics_in_result, dict) else {}
        if isinstance(clusters, dict) and clusters:
            for key, rows in clusters.items():
                try:
                    pd.DataFrame(rows).to_excel(wr, sheet_name=f"Cluster_{key}", index=False)
                except Exception:
                    pass

# =========================
# UI: íŒŒì¼ ì—…ë¡œë“œ + ì‹¤í–‰
# =========================
uploaded_file = st.file_uploader("ì›ë³¸ QA ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"])
# ë¡œê·¸ ì…ë ¥ì€ ë¹„í™œì„±(í•„ìš” ì‹œ ì£¼ì„ í•´ì œ)
log_files = None  # st.file_uploader("Logcat íŒŒì¼ (.txt/.log/.zip, ë‹¤ì¤‘)", type=["txt","log","zip"], accept_multiple_files=True)
st.caption("â€» Logcat ë¶„ì„ì€ í˜„ì¬ ë¹„í™œì„±í™” ìƒíƒœì…ë‹ˆë‹¤.")

if uploaded_file:
    with step_status("ì—‘ì…€ ë¡œë“œ"):
        xls = pd.ExcelFile(uploaded_file, engine="openpyxl")
        diag_dump("ì‹œíŠ¸ ëª©ë¡", xls.sheet_names)

    with step_status("í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ìë™ê°ì§€"):
        test_candidates = find_test_sheet_candidates(xls)
        if not test_candidates:
            test_candidates = xls.sheet_names
        diag_dump("ê°ì§€ëœ í›„ë³´ ì‹œíŠ¸", test_candidates)

    st.subheader("1. í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ì„ íƒ")
    test_sheets_selected = st.multiselect(
        "ìë™ ê°ì§€ëœ í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ì¤‘ ë¶„ì„ ëŒ€ìƒ ì„ íƒ",
        options=test_candidates,
        default=test_candidates[:2]
    )
    if not test_sheets_selected:
        st.error("âŒ ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
        st.stop()

    st.subheader("2. ìŠ¤í™ ì‹œíŠ¸ ì„ íƒ (ë””ë°”ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸)")
    default_spec = [s for s in ["AOS_Device_List", "iOS_Device_List"] if s in xls.sheet_names]
    spec_sheets_selected = st.multiselect(
        "ìŠ¤í™(GPU/Chipset/OS/Rank ë“±) í¬í•¨ ì‹œíŠ¸ ì„ íƒ",
        options=xls.sheet_names,
        default=default_spec
    )
    st.markdown("---")

    if st.button("ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±", type="primary"):
        # ìƒíƒœ ì´ˆê¸°í™”
        log_summary = {}
        log_hypotheses = []
        clusters_meta = {}

        # 3) Fail + ì…€ ì½”ë©˜íŠ¸ ì¶”ì¶œ
        with step_status("Fail + ì…€ ì½”ë©˜íŠ¸ ì¶”ì¶œ"):
            wb = openpyxl.load_workbook(uploaded_file, data_only=True)
            df_issue = extract_comments_as_dataframe(wb, test_sheets_selected)
            if df_issue is None or df_issue.empty:
                st.error("âŒ Fail + ì½”ë©˜íŠ¸ê°€ í¬í•¨ëœ í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
                st.stop()

        # 4) ë¹„ê³ /Notes ë³‘í•©
        with step_status("ë¹„ê³ /Notes ë³‘í•©"):
            df_issue = enrich_with_column_comments(xls, test_sheets_selected[0], df_issue)
            diag_dump("ë³‘í•© ê²°ê³¼ ìƒ˜í”Œ", df_issue.head(10))

        # 5) ìŠ¤í™ ë³‘í•©
        with step_status("ìŠ¤í™ ë³‘í•©"):
            df_final = df_issue.copy()
            match_rate = 0.0
            if spec_sheets_selected:
                try:
                    spec_frames = [load_std_spec_df(xls, s) for s in spec_sheets_selected]
                    df_spec_all = pd.concat(spec_frames, ignore_index=True)
                    df_spec_all = df_spec_all.drop_duplicates(subset=["model_norm"], keep="first")

                    df_final["model_norm"] = df_final["Device(Model)"].apply(normalize_model_name_strict)
                    df_final = pd.merge(df_final, df_spec_all, on="model_norm", how="left")

                    for col in ["GPU", "ì œì¡°ì‚¬", "Chipset", "RAM", "OS", "Rank", "Model"]:
                        cx, cy = f"{col}_x", f"{col}_y"
                        if cx in df_final.columns and cy in df_final.columns:
                            df_final[col] = df_final[cx].where(df_final[cx].notna(), df_final[cy])
                            df_final.drop(columns=[cx, cy], inplace=True)
                        elif cx in df_final.columns:
                            df_final.rename(columns={cx: col}, inplace=True)
                        elif cy in df_final.columns:
                            df_final.rename(columns={cy: col}, inplace=True)
                    if "GPU" in df_final.columns:
                        df_final["GPU"] = df_final["GPU"].apply(normalize_gpu)
                    if "Chipset" in df_final.columns:
                        df_final["Chipset"] = df_final["Chipset"].apply(normalize_chipset)

                    if "GPU" in df_final.columns:
                        matched = int(df_final["GPU"].notna().sum())
                        match_rate = round(matched / len(df_final) * 100, 1)
                        st.success(f"ìŠ¤í™ ë§¤ì¹­ ê²°ê³¼: {matched} / {len(df_final)} ê±´ ({match_rate}%)")
                except Exception as e:
                    st.warning(f"ìŠ¤í™ ë³‘í•© ì¤‘ ì¼ë¶€ ì˜¤ë¥˜: {e}")

        # 6) Logcat (ì˜µì…˜)
        with step_status("Logcat ë¶„ì„"):
            if log_files:
                log_summary = load_and_summarize_logcat_files(log_files)
                st.info(f"Logcat ìš”ì•½: {log_summary.get('log_summary','-')}")
                log_hypotheses = infer_root_causes_from_logs(log_summary.get("log_summary", ""))
                diag_dump("ë¡œê·¸ ê·¼ë³¸ ì›ì¸ ê°€ì„¤", log_hypotheses)
            else:
                st.info("ë¡œê·¸ íŒŒì¼ ì—†ìŒ. Logcat ë¶„ì„ ìƒëµ.")

        # 7) êµ°ì§‘ ë‹¤ì´ì œìŠ¤íŠ¸
        with step_status("êµ°ì§‘ ë‹¤ì´ì œìŠ¤íŠ¸ ìƒì„±"):
            cluster_digests = make_cluster_digests(df_final, min_group=2, per_cluster_max_samples=8)
            diag_dump("êµ°ì§‘ ë‹¤ì´ì œìŠ¤íŠ¸", cluster_digests[:10])
            # êµ°ì§‘ ë©”íƒ€ ê¸°ë¡(ì—‘ì…€ Cluster_* ì‹œíŠ¸ ìš©)
            clusters_meta = {
                "spec": [{"axis": d["axis"], "value": d["value"], "size": d["size"]}
                         for d in cluster_digests if d["axis"] in ["GPU","Chipset","OS"]],
                "tags": [{"axis": d["axis"], "value": d["value"], "size": d["size"]}
                         for d in cluster_digests if d["axis"] == "issue_tag"],
            }

        # 8) êµ°ì§‘ ë‹¨ìœ„ GPT ì½”ë©˜íŠ¸ ìƒì„±
        with step_status("GPT ì½”ë©˜íŠ¸ ìƒì„±(êµ°ì§‘ ë‹¨ìœ„)"):
            issues = []
            for dig in cluster_digests:
                try:
                    draft = write_issue_with_gpt(client, dig, log_hypotheses=log_hypotheses)
                except Exception as e:
                    # ì‹¤íŒ¨ ì‹œ ìµœì†Œí•œì˜ ê·œì¹™ ê¸°ë°˜ ë°±ì—…
                    draft = {
                        "title": f"{dig['axis']}:{dig['value']} êµ°ì§‘ ì´ìŠˆ",
                        "symptom": "ê³µí†µ í˜„ìƒ ë°œìƒ",
                        "evidence": dig.get("evidence_comments", [])[:3],
                        "impact": "ì‚¬ìš©ì ê²½í—˜ ì €í•˜",
                        "cause": "ì›ì¸ ì¶”ì • í•„ìš”(ë¡œê·¸/ë¦¬í”„ë¡œ ë³´ê°•)",
                        "recommendation": "ì¬í˜„ ë¡œê·¸ í™•ë³´ ë° ì¡°ê±´í™”ëœ ë¦¬í”„ë¡œ ì ìš©",
                        "priority": "P1"
                    }
                issues.append({
                    "title": draft.get("title", f"{dig['axis']}:{dig['value']} êµ°ì§‘ ì´ìŠˆ"),
                    "symptom": draft.get("symptom",""),
                    "reproduction": "êµ°ì§‘ ë‚´ ê³µí†µ ì¡°ê±´ì—ì„œ ë°˜ë³µ ì¬í˜„ë¨",
                    "evidence": draft.get("evidence", dig.get("evidence_comments", [])[:3]),
                    "impact": draft.get("impact","ì‚¬ìš©ì ê²½í—˜ ì €í•˜"),
                    "priority": draft.get("priority","P1"),
                    "cause": draft.get("cause","ì›ì¸ ì¶”ì • í•„ìš”"),
                    "recommendation": draft.get("recommendation","ì¬í˜„ ë¡œê·¸ í™•ë³´ ë° ë¦¬í”„ë¡œ ì ìš©"),
                })

            # ë¯¸êµ°ì§‘(ë‹¨ì¼Â·í©ì–´ì§„ ê²ƒë“¤)ë„ ëˆ„ë½ ì—†ì´ ì¶”ê°€
            clustered_idx = set()
            # ê°„ë‹¨í™”ë¥¼ ìœ„í•´ example_rowsì˜ indexë¥¼ ì“°ì§€ ì•Šê³ , cluster ì»¤ë²„ì—ì„œ ì œì™¸ëœ ì¶”ì • ê°œìˆ˜ëŠ” ìƒëµ
            # í•„ìš”í•˜ë©´ df_final.indexë¥¼ ì¶”ì í•˜ëŠ” ë¡œì§ì„ ì¶”ê°€í•˜ì„¸ìš”.

        # 9) ê²°ê³¼ ì¡°ë¦½ ë° ì €ì¥
        with step_status("ë¦¬í¬íŠ¸ ì €ì¥"):
            result = {
                "summary": "êµ°ì§‘ ë‹¨ìœ„ë¡œ ìƒì„±ëœ ë°ì´í„° ê¸°ë°˜ ì½”ë©˜íŠ¸ì…ë‹ˆë‹¤.",
                "issues": issues,
                "device_risks": [
                    {"device_model_or_combo": ", ".join(d["devices"][:6]) or "(ë‹¤ìˆ˜)",
                     "reason": f"{d['axis']}:{d['value']} êµ°ì§‘ì—ì„œ ì¬í˜„ ë¹ˆë„ ë†’ìŒ",
                     "impact": "ëŒ€ìƒ êµ°ì§‘ ì‚¬ìš©ì ì²´ê° ì˜í–¥ í¼"}
                    for d in cluster_digests if d["size"] >= 3
                ],
                "actions": [],
                "release_decision": "Conditional",
                "conditions": "ìƒìœ„ êµ°ì§‘ íŒ¨ì¹˜ ì ìš© ë° ì¬í…ŒìŠ¤íŠ¸ í†µê³¼ ì‹œ ë°°í¬",
                "metrics": {"clusters": clusters_meta}
            }
            output = "QA_Report.xlsx"
            write_excel_report(result, df_final, output)
            st.success("âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            with open(output, "rb") as f:
                st.download_button("ğŸ“Š Excel ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ", f.read(), file_name=output)

        # í‘œì‹œìš© ìƒ˜í”Œ
        st.success(f"{len(df_final)}ê°œì˜ 'Fail' í•­ëª© ë¶„ì„ ì™„ë£Œ.")
        st.dataframe(df_final.head(20), use_container_width=True)
