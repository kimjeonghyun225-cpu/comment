# -*- coding: utf-8 -*-
# Streamlit ì•±: QA ê²°ê³¼ ìë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸° (ìµœì¢…)

import os, re, io, time, unicodedata
from contextlib import contextmanager
import pandas as pd
import openpyxl
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI
from typing import List, Dict, Any, Optional

# ============= ê¸°ë³¸ ì„¤ì • =============
load_dotenv()
st.set_page_config(page_title="QA ê²°ê³¼ ìë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°", layout="wide")
st.title(":bar_chart: QA ê²°ê³¼ ìë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°")

api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))
if not api_key:
    st.error("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. st.secrets ë˜ëŠ” .envì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    st.stop()
client = OpenAI(api_key=api_key)

@contextmanager
def step_status(title: str):
    with st.status(title, expanded=False) as status:
        t0 = time.time()
        try:
            yield status
            status.update(label=f"{title} - ì™„ë£Œ ({time.time()-t0:.2f}s)", state="complete", expanded=False)
        except Exception as e:
            status.update(label=f"{title} - ì‹¤íŒ¨: {e}", state="error", expanded=True)
            raise

def diag_dump(label: str, obj): 
    with st.expander(f"ğŸ” ì§„ë‹¨ ë³´ê¸°: {label}", expanded=False):
        st.write(obj)

# ì„¸ì…˜ ì´ˆê¸°í™”
if st.button("ğŸ”„ ì„¸ì…˜ ì´ˆê¸°í™”"):
    st.session_state.clear()
    st.rerun()

st.caption("â€» Logcat ë¶„ì„ì€ í˜„ì¬ ë¹„í™œì„±í™” ìƒíƒœì…ë‹ˆë‹¤.")
uploaded_file = st.file_uploader("ì›ë³¸ QA ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"])

if not uploaded_file:
    st.stop()

# ============= ì—‘ì…€ ë¡œë“œ & ì‹œíŠ¸ ì„ íƒ =============
data = uploaded_file.read()
with step_status("ì—‘ì…€ ë¡œë“œ"):
    xls = pd.ExcelFile(io.BytesIO(data), engine="openpyxl")
    diag_dump("ì‹œíŠ¸ ëª©ë¡", xls.sheet_names)

with step_status("í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ìë™ê°ì§€"):
    test_candidates = find_test_sheet_candidates(xls)
    diag_dump("ê°ì§€ëœ í›„ë³´ ì‹œíŠ¸", test_candidates)

st.subheader("1. í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ì„ íƒ")
test_sheets_selected = st.multiselect(
    "ìë™ ê°ì§€ëœ í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ì¤‘ ë¶„ì„ ëŒ€ìƒ ì„ íƒ",
    options=test_candidates,
    default=test_candidates[:2]
)
if not test_sheets_selected:
    st.error("âŒ ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
    st.stop()

st.subheader("2. ìŠ¤í™ ì‹œíŠ¸ ì„ íƒ (ë””ë°”ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸)")
default_spec = [s for s in ["AOS_Device_List", "iOS_Device_List"] if s in xls.sheet_names]
spec_sheets_selected = st.multiselect(
    "ìŠ¤í™(Chipset, GPU, OS, Rank ë“±) í¬í•¨ ì‹œíŠ¸ ì„ íƒ",
    options=xls.sheet_names,
    default=default_spec
)
st.markdown("---")

# ============= ì‹¤í–‰ =============
if not st.button("ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±", type="primary"):
    st.stop()

log_hypotheses, clusters, evidence_links = [], {}, []

# 3) Fail + ì½”ë©˜íŠ¸ ì¶”ì¶œ (ë¼ë²¨í–‰â†’Failì—´ ì„¸ë¡œì¶”ì¶œ, ë³‘í•©ì…€ ë³´ì •)
with step_status("Fail + ì…€ ì½”ë©˜íŠ¸ ì¶”ì¶œ"):
    wb = openpyxl.load_workbook(io.BytesIO(data), data_only=True)
    df_issue = extract_comments_as_dataframe(wb, test_sheets_selected)
    diag_dump("ì¶”ì¶œ ìƒ˜í”Œ", df_issue.head(12))
    if df_issue.empty:
        st.warning("âŒ Fail+ì½”ë©˜íŠ¸ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤(ì…€ ì½”ë©˜íŠ¸ ê¸°ì¤€).")
        st.stop()

# 4) ë¹„ê³ /Notes ë³‘í•© (ì„ íƒ ì‹œíŠ¸ ì „ë¶€)
with step_status("ë¹„ê³ /Notes ë³‘í•©"):
    for _sheet in test_sheets_selected:
        df_issue = enrich_with_column_comments(xls, _sheet, df_issue)
    diag_dump("ë¹„ê³  ë³‘í•© ê²°ê³¼", df_issue.head(12))

# 5) ìŠ¤í™ ë³‘í•© (ëª¨ë¸ëª… ì •ê·œí™” í›„ Join)
df_final = df_issue.copy()
match_rate = 0.0
if spec_sheets_selected:
    with step_status("ìŠ¤í™ ë³‘í•©"):
        def _stdcols(df: pd.DataFrame) -> pd.DataFrame:
            orig = list(df.columns)
            norm = [unicodedata.normalize("NFKC", str(c)) for c in orig]
            norm = [re.sub(r"[\s\-\_/()\[\]{}:+Â·âˆ™â€¢]", "", s).lower() for s in norm]
            synonyms = {
                r"^(model|device|ì œí’ˆëª…|ì œí’ˆ|ëª¨ë¸ëª…|ëª¨ë¸)$": "Model",
                r"^(maker|manufacturer|brand|oem|ì œì¡°ì‚¬|ë²¤ë”)$": "ì œì¡°ì‚¬",
                r"^(gpu|ê·¸ë˜í”½|ê·¸ë˜í”½ì¹©|ê·¸ë˜í”½ìŠ¤|ê·¸ë˜í”½í”„ë¡œì„¸ì„œ)$": "GPU",
                r"^(chipset|soc|ap|cpu|processor)$": "Chipset",
                r"^(ram|ë©”ëª¨ë¦¬)$": "RAM",
                r"^(os|osversion|android|ios|íŒì›¨ì–´|ì†Œí”„íŠ¸ì›¨ì–´ë²„ì „)$": "OS",
                r"^(rank|rating|ratinggrade|ë“±ê¸‰)$": "Rank",
            }
            col_map = {}
            for n, o in zip(norm, orig):
                mapped = None
                for pat, std in synonyms.items():
                    if re.search(pat, n):
                        mapped = std; break
                col_map[o] = mapped or o
            return df.rename(columns=col_map)

        frames = []
        for sname in spec_sheets_selected:
            dfp = pd.read_excel(xls, sheet_name=sname, engine="openpyxl")
            dfp = _stdcols(dfp)
            model_col = "Model" if "Model" in dfp.columns else None
            if not model_col:
                for c in dfp.columns:
                    n = re.sub(r"[\s\-\_/()\[\]{}:+Â·âˆ™â€¢]", "", unicodedata.normalize("NFKC", str(c))).lower()
                    if re.search(r"^(model|device|ì œí’ˆëª…|ì œí’ˆ|ëª¨ë¸ëª…|ëª¨ë¸)$", n): model_col = c; break
            if not model_col:
                continue
            dfp["model_norm"] = dfp[model_col].apply(normalize_model_name_strict)
            keep = ["model_norm"] + [c for c in ["GPU","ì œì¡°ì‚¬","Chipset","RAM","OS","Rank","Model","CPU"] if c in dfp.columns]
            frames.append(dfp[keep])

        if frames:
            df_spec_all = pd.concat(frames, ignore_index=True).drop_duplicates("model_norm", keep="first")
            df_final["model_norm"] = df_final["Device(Model)"].apply(normalize_model_name_strict)
            df_final = pd.merge(df_final, df_spec_all, on="model_norm", how="left")

            if "Chipset" not in df_final.columns and "CPU" in df_final.columns:
                df_final["Chipset"] = df_final["CPU"]

            for col in ["GPU","ì œì¡°ì‚¬","Chipset","RAM","OS","Rank","Model"]:
                cx, cy = f"{col}_x", f"{col}_y"
                if cx in df_final.columns and cy in df_final.columns:
                    df_final[col] = df_final[cx].where(df_final[cx].notna(), df_final[cy])
                    df_final.drop(columns=[cx, cy], inplace=True)
                elif cx in df_final.columns:
                    df_final.rename(columns={cx: col}, inplace=True)
                elif cy in df_final.columns:
                    df_final.rename(columns={cy: col}, inplace=True)

            if "GPU" in df_final.columns:
                matched = int(df_final["GPU"].notna().sum())
                match_rate = round(matched / len(df_final) * 100, 1)
                st.success(f"ìŠ¤í™ ë§¤ì¹­ ê²°ê³¼: {matched} / {len(df_final)} ê±´ ({match_rate}%)")

# 6) ìê°€ì§„ë‹¨
with step_status("ëª¨ë“ˆ ìê°€ì§„ë‹¨"):
    diag = self_check(df_final)
    diag_dump("self_check", diag)
    if not diag["row_ok"]:
        st.error("âŒ ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ. ì¤‘ë‹¨.")
        st.stop()

# 7) ì½”ë©˜íŠ¸ ì •ê·œí™”/íƒœê¹…
with step_status("ì½”ë©˜íŠ¸ ì •ê·œí™” / íƒœê¹…"):
    def _jamo_norm(s: str) -> str:
        if s is None: return ""
        t = unicodedata.normalize("NFKC", str(s))
        t = re.sub(r"[^0-9a-zA-Zê°€-í£\s\-_+/.:]", " ", t)
        t = re.sub(r"\s+", " ", t).strip().lower()
        return t

    ISSUE_TAG_PATTERNS = [
        ("punch_hole", r"(í€ì¹˜í™€|punch[\s\-]?hole|hole[-\s]?camera)"),
        ("notch", r"(ë…¸ì¹˜|notch)"),
        ("rotation", r"(íšŒì „|ê°€ë¡œì „í™˜|ì„¸ë¡œì „í™˜|landscape|portrait|rotate)"),
        ("aspect_ratio", r"(í™”ë©´ë¹„|ë¹„ìœ¨|aspect\s?ratio)"),
        ("resolution", r"(í•´ìƒë„|resolution)"),
        ("cutout", r"(ì»·ì•„ì›ƒ|cutout)"),
        ("install", r"(ì„¤ì¹˜\s?ë¶ˆê°€|ì„¤ì¹˜ì˜¤ë¥˜|install\s?fail|íŒ¨í‚¤ì§€\s?ì˜¤ë¥˜|apk\s?ì„¤ì¹˜)"),
        ("permission", r"(ê¶Œí•œ|permission)"),
        ("login", r"(ë¡œê·¸ì¸|login|oauth|ì¸ì¦|auth)"),
        ("storage", r"(ì €ì¥ê³µê°„|storage|sd\s?card|ê¶Œí•œ\s?ê±°ë¶€)"),
        ("input_lag", r"(ì…ë ¥\s?ì§€ì—°|í„°ì¹˜\s?ì§€ì—°|ui\s?ì§€ì—°|input\s?lag|ui\s?lag)"),
        ("keyboard", r"(í‚¤ë³´ë“œ|ime|keyboard)"),
        ("ui_scaling", r"(ui\s?ìŠ¤ì¼€ì¼|í™•ëŒ€|ì¶•ì†Œ|dpi|density)"),
        ("render_artifact", r"(ì•„í‹°íŒ©íŠ¸|ê¹¨ì§|ì”ìƒ|í…Œì–´ë§|ê¸€ë¦¬ì¹˜|artifact|glitch|tearing)"),
        ("black_screen", r"(ê²€ì€\s?í™”ë©´|black\s?screen)"),
        ("white_screen", r"(í•˜ì–€\s?í™”ë©´|white\s?screen)"),
        ("crash", r"(í¬ë˜ì‹œ|fatal exception|ê°•ì œì¢…ë£Œ|crash)"),
        ("network", r"(ë„¤íŠ¸ì›Œí¬|network|ssl|handshake|timeout|unknownhost)"),
        ("audio", r"(ì†Œë¦¬|ì˜¤ë””ì˜¤|audio|ë¬´ìŒ|ë³¼ë¥¨)"),
        ("camera", r"(ì¹´ë©”ë¼|camera)"),
        ("thermal", r"(ì¨ë©€|ë°œì—´|thermal|throttl)"),
        ("fps", r"(í”„ë ˆì„|fps)"),
    ]
    def tag_issue_comment(comment: str) -> list:
        s = _jamo_norm(comment)
        tags = []
        for tag, pat in ISSUE_TAG_PATTERNS:
            if re.search(pat, s, re.I): tags.append(tag)
        return list(dict.fromkeys(tags))

    if "comment_text" not in df_final.columns: df_final["comment_text"] = ""
    df_final["comment_norm"] = df_final["comment_text"].fillna("").astype(str).apply(_jamo_norm)
    df_final["issue_tags"]   = df_final["comment_text"].fillna("").astype(str).apply(tag_issue_comment)
    diag_dump("íƒœê¹… ìƒ˜í”Œ", df_final[["Device(Model)","GPU","Chipset","OS","comment_text","issue_tags"]].head(15))

# 8) êµ°ì§‘ ì‚°ì¶œ
with step_status("êµ°ì§‘(Cluster) í†µê³„ ì‚°ì¶œ"):
    if "Chipset" not in df_final.columns and "CPU" in df_final.columns:
        df_final["Chipset"] = df_final["CPU"]

    if "GPU" not in df_final.columns: df_final["GPU"] = None
    if "Chipset" not in df_final.columns: df_final["Chipset"] = None

    df_final["GPU"] = (
        df_final["GPU"].astype(str)
        .str.replace(r"\bPower\s*VR\b", "PowerVR", regex=True)
        .str.replace(r"\bIMG\s+GE", "PowerVR GE", regex=True)
        .str.replace(r"\bGE(\d+)\b", r"PowerVR GE\1", regex=True)
    )

    def _cluster_counts(df, col, topn=15):
        if col not in df.columns: return pd.DataFrame(columns=[col,"count"])
        vc = df[col].fillna("(ë¯¸ê¸°ì¬)").astype(str).str.strip().value_counts().head(topn)
        return vc.reset_index().rename(columns={"index":col, 0:"count"})

    cluster_gpu  = _cluster_counts(df_final, "GPU")
    cluster_chip = _cluster_counts(df_final, "Chipset")

    clusters = {
        "by_gpu": cluster_gpu.to_dict(orient="records"),
        "by_chipset": cluster_chip.to_dict(orient="records"),
    }

    feat_rows = []
    for idx, r in df_final.iterrows():
        for t in (r.get("issue_tags") or []):
            feat_rows.append({
                "tag": t, "row_idx": idx,
                "device": str(r.get("Device(Model)", "")),
                "gpu": str(r.get("GPU", "")),
                "chipset": str(r.get("Chipset","")),
                "os": str(r.get("OS","")),
                "comment": str(r.get("comment_text",""))
            })
    feat_df = pd.DataFrame(feat_rows)
    clusters_feature_detailed, by_issue_tag = [], []
    if not feat_df.empty:
        g = (feat_df.groupby("tag")
                .agg(count=("row_idx","size"),
                     repr_models=("device", lambda s: list(pd.Series(s).dropna().unique())[:3]),
                     evidence_rows=("row_idx", list))
                .sort_values("count", ascending=False).reset_index())
        by_issue_tag = g[["tag","count"]].rename(columns={"tag":"value"}).to_dict(orient="records")
        for _, row in g.iterrows():
            ev = []
            for ridx in row["evidence_rows"][:6]:
                rr = df_final.loc[ridx]
                ev.append({
                    "row_idx": int(ridx),
                    "device": str(rr.get("Device(Model)","")),
                    "os": str(rr.get("OS","")),
                    "comment": str(rr.get("comment_text",""))[:180]
                })
            clusters_feature_detailed.append({
                "feature_tag": row["tag"],
                "pattern": row["tag"],
                "count": int(row["count"]),
                "repr_models": row["repr_models"],
                "evidence_rows": ev,
                "singleton": (int(row["count"]) == 1)
            })

    diag_dump("GPU/Chipset êµ°ì§‘", clusters)
    diag_dump("Feature êµ°ì§‘ ìš”ì•½", by_issue_tag)
    diag_dump("Feature êµ°ì§‘ ìƒì„¸(ì¼ë¶€)", clusters_feature_detailed[:3])

# 8.5) í† í° ì ˆê°ìš© ì••ì¶• ìƒ˜í”Œ
def _compact_str(s, n=160):
    s = (str(s or "")).strip()
    return (s[:n] + "â€¦") if len(s) > n else s

def make_compact_sample(df: pd.DataFrame, per_tag=30, per_gpu=20, per_chip=20, max_rows=450):
    keep = [c for c in ["Sheet","Device(Model)","GPU","Chipset","OS","comment_text","issue_tags"] if c in df.columns]
    slim = df[keep].copy()
    slim["comment_text"] = slim["comment_text"].map(lambda x: _compact_str(x, 180))
    slim["__dedup_key__"] = (
        slim["Device(Model)"].astype(str).str.strip().str.lower()
        + "||" + slim["comment_text"].astype(str).str.strip().str.lower()
    )
    slim = slim.drop_duplicates("__dedup_key__")

    out = []
    if "issue_tags" in slim.columns:
        tag_order = ["crash","black_screen","white_screen","render_artifact","rotation",
                     "aspect_ratio","ui_scaling","resolution","permission","install",
                     "input_lag","fps","thermal","network","audio","camera","notch","punch_hole"]
        for t in tag_order:
            sub = slim[slim["issue_tags"].astype(str).str.contains(t, regex=False, na=False)].head(per_tag)
            out.append(sub)
    if "GPU" in slim.columns:
        for g in slim["GPU"].fillna("(ë¯¸ê¸°ì¬)").value_counts().head(10).index.tolist():
            out.append(slim[slim["GPU"] == g].head(per_gpu))
    if "Chipset" in slim.columns:
        for c in slim["Chipset"].fillna("(ë¯¸ê¸°ì¬)").value_counts().head(10).index.tolist():
            out.append(slim[slim["Chipset"] == c].head(per_chip))

    compact = pd.concat(out, ignore_index=True).drop_duplicates("__dedup_key__")
    compact = compact.head(max_rows).drop(columns=["__dedup_key__"], errors="ignore")
    return compact

compact_issues = make_compact_sample(df_final, per_tag=30, per_gpu=20, per_chip=20, max_rows=450)

# 9) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
metrics = {
    "total_fail_issues": len(df_final),
    "clusters": clusters,
    "by_issue_tag": by_issue_tag,
    "clusters_feature_detailed": clusters_feature_detailed,
    "log_hypotheses": log_hypotheses
}
deltas, evidence_links = {}, []

sp = build_system_prompt()
up = build_user_prompt(
    project="UNKNOWN_PROJECT",
    version="UNKNOWN_VERSION",
    metrics=metrics,
    deltas=deltas,
    evidence_links=evidence_links,
    sample_issues=compact_issues,
    max_rows=500
)

# 10) OpenAI í˜¸ì¶œ
with st.spinner("GPTê°€ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
    max_retries, wait = 3, 20
    result, last_error = None, None
    for attempt in range(max_retries):
        try:
            resp = client.chat.completions.create(
                model="gpt-4o",
                temperature=0.1,
                top_p=0.9,
                messages=[{"role":"system","content":sp},{"role":"user","content":up}],
                response_format={"type":"json_object"}
            )
            raw = resp.choices[0].message.content
            result = parse_llm_json(raw); result["metrics"] = metrics
            diag_dump("LLM ì›ë¬¸(ìš”ì•½)", raw[:3000])
            break
        except Exception as e:
            last_error = e
            if "429" in str(e) or "rate_limit_exceeded" in str(e).lower():
                if attempt < max_retries-1:
                    st.warning(f"429 ê°ì§€, ì¬ì‹œë„ {attempt+1}/{max_retries}")
                    time.sleep(wait); wait *= 2
                    continue
            st.error(f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            st.stop()
    if result is None:
        st.error(f"OpenAI ìµœì¢… ì‹¤íŒ¨: {last_error}")
        st.stop()

# 11) ë¦¬í¬íŠ¸ ìƒì„±
try:
    output = "QA_Report.xlsx"
    write_excel_report(result, df_final, output)
    st.success("âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
    with open(output, "rb") as f:
        st.download_button("ğŸ“Š Excel ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ", f.read(), file_name=output)
except Exception as e:
    st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")

