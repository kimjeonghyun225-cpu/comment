# -*- coding: utf-8 -*-
# Streamlit ì•±: QA ê²°ê³¼ ìë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸° (ìµœì¢…)

import os, re, io, time, unicodedata
from contextlib import contextmanager
from typing import List, Dict, Any, Optional

import pandas as pd
import openpyxl
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

from qa_patch_module import (
    find_test_sheet_candidates,
    extract_comments_as_dataframe_dual,
    enrich_with_column_comments,
    build_system_prompt, build_user_prompt,
    parse_llm_json, write_excel_report,
    self_check, normalize_model_name_strict,
    load_threaded_comments_map_from_bytes  # â˜… ìŠ¤ë ˆë“œ ëŒ“ê¸€ íŒŒì„œ
)

# ============= ê¸°ë³¸ ì„¤ì • =============
load_dotenv()
st.set_page_config(page_title="QA ê²°ê³¼ ìë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°", layout="wide")
st.title(":bar_chart: QA ê²°ê³¼ ìë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°")

api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))
if not api_key:
    st.error("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. st.secrets ë˜ëŠ” .envì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    st.stop()
client = OpenAI(api_key=api_key)

@contextmanager
def step_status(title: str):
    with st.status(title, expanded=False) as status:
        t0 = time.time()
        try:
            yield status
            status.update(label=f"{title} - ì™„ë£Œ ({time.time()-t0:.2f}s)", state="complete", expanded=False)
        except Exception as e:
            status.update(label=f"{title} - ì‹¤íŒ¨: {e}", state="error", expanded=True)
            raise

def diag_dump(label: str, obj):
    with st.expander(f"ğŸ” ì§„ë‹¨ ë³´ê¸°: {label}", expanded=False):
        st.write(obj)

# ì„¸ì…˜ ì´ˆê¸°í™”
if st.button("ğŸ”„ ì„¸ì…˜ ì´ˆê¸°í™”"):
    st.session_state.clear()
    st.rerun()

st.caption("â€» Logcat ë¶„ì„ì€ í˜„ì¬ ë¹„í™œì„±í™” ìƒíƒœì…ë‹ˆë‹¤.")
uploaded_file = st.file_uploader("ì›ë³¸ QA ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"])

if not uploaded_file:
    st.stop()

# ============= ì—‘ì…€ ë¡œë“œ & ì‹œíŠ¸ ì„ íƒ =============
data = uploaded_file.read()
with step_status("ì—‘ì…€ ë¡œë“œ"):
    try:
        xls = pd.ExcelFile(io.BytesIO(data), engine="openpyxl")
        diag_dump("ì‹œíŠ¸ ëª©ë¡", xls.sheet_names)
    except Exception as e:
        st.error(f"ì—‘ì…€ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        st.stop()

with step_status("í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ìë™ê°ì§€"):
    try:
        test_candidates = find_test_sheet_candidates(xls)
        if not test_candidates:
            test_candidates = xls.sheet_names
        diag_dump("ê°ì§€ëœ í›„ë³´ ì‹œíŠ¸", test_candidates)
    except Exception as e:
        st.error(f"í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ê°ì§€ ì˜¤ë¥˜: {e}")
        test_candidates = xls.sheet_names if hasattr(xls, 'sheet_names') else []

st.subheader("1. í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ì„ íƒ")
test_sheets_selected = st.multiselect(
    "ìë™ ê°ì§€ëœ í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ì¤‘ ë¶„ì„ ëŒ€ìƒ ì„ íƒ",
    options=test_candidates,
    default=test_candidates[:2]
)
if not test_sheets_selected:
    st.error("âŒ ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
    st.stop()

st.subheader("2. ìŠ¤í™ ì‹œíŠ¸ ì„ íƒ (ë””ë°”ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸)")
default_spec = [s for s in ["AOS_Device_List", "iOS_Device_List"] if s in xls.sheet_names]
spec_sheets_selected = st.multiselect(
    "ìŠ¤í™(Chipset, GPU, OS, Rank ë“±) í¬í•¨ ì‹œíŠ¸ ì„ íƒ",
    options=xls.sheet_names,
    default=default_spec
)
st.markdown("---")

# ============= ì‹¤í–‰ =============
if not st.button("ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±", type="primary"):
    st.stop()

log_hypotheses, clusters, evidence_links = [], {}, []

# 3) Fail + ì½”ë©˜íŠ¸ ì¶”ì¶œ (ë¼ë²¨í–‰â†’Failì—´ ì„¸ë¡œì¶”ì¶œ, ë³‘í•©ì…€ ë³´ì •, ìˆ˜ì‹/ìŠ¤ë ˆë“œëŒ“ê¸€ ëŒ€ì‘)
with step_status("Fail + ì…€ ì½”ë©˜íŠ¸ ì¶”ì¶œ"):
    try:
        wb_comm = openpyxl.load_workbook(io.BytesIO(data), data_only=False)
        wb_val  = openpyxl.load_workbook(io.BytesIO(data), data_only=True)

        available = set(wb_comm.sheetnames) & set(wb_val.sheetnames)
        valid_sheets = [s for s in test_sheets_selected if s in available]
        if not valid_sheets:
            st.error(f"ì„ íƒí•œ ì‹œíŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥: {sorted(list(available))}")
            st.stop()

        # â˜… ìƒˆ ëŒ“ê¸€(ìŠ¤ë ˆë“œ)ê¹Œì§€ ì½ì–´ì„œ ë³´ê°•
        threaded_map = load_threaded_comments_map_from_bytes(data)

        df_issue = extract_comments_as_dataframe_dual(
            wb_comm, wb_val, valid_sheets, threaded_map=threaded_map
        )
        diag_dump("ì¶”ì¶œ ìƒ˜í”Œ", df_issue.head(12))

        if df_issue.empty:
            st.warning("âŒ Fail+ì½”ë©˜íŠ¸ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤(ë©”ëª¨/ëŒ“ê¸€ ë¯¸ê²€ì¶œ).")
            st.info("ì—‘ì…€ì—ì„œ í•´ë‹¹ ì…€ì— ì‹¤ì œ ì½”ë©˜íŠ¸ê°€ ì¡´ì¬í•˜ëŠ”ì§€(ìƒˆ ëŒ“ê¸€/ë©”ëª¨), ë³´í˜¸/ìˆ¨ê¹€ ìƒíƒœê°€ ì•„ë‹Œì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.")
            st.stop()
    except Exception as e:
        st.error(f"ì½”ë©˜íŠ¸ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        st.stop()

# 4) ë¹„ê³ /Notes ë³‘í•© (ì„ íƒ ì‹œíŠ¸ ì „ë¶€)
with step_status("ë¹„ê³ /Notes ë³‘í•©"):
    for _sheet in test_sheets_selected:
        df_issue = enrich_with_column_comments(xls, _sheet, df_issue)
    diag_dump("ë¹„ê³  ë³‘í•© ê²°ê³¼", df_issue.head(12))

# 5) ìŠ¤í™ ë³‘í•© (ëª¨ë¸ëª… ì •ê·œí™” í›„ Joinâ€”í—¤ë” ìë™íƒì§€ + ë¶€ë¶„ì¼ì¹˜ ë°±ì—…)
df_final = df_issue.copy()
match_rate = 0.0

if spec_sheets_selected:
    with step_status("ìŠ¤í™ ë³‘í•©"):
        # ---------- ê³µí†µ ìœ í‹¸ ----------
        def _norm_hdr(s: str) -> str:
            s = unicodedata.normalize("NFKC", str(s))
            s = re.sub(r"[\s\-\_/()\[\]{}:+Â·âˆ™â€¢]", "", s).lower()
            return s

        def find_header_row_for_spec(xls, sheet, max_scan_rows=20):
            """ìŠ¤í™ ì‹œíŠ¸ì—ì„œ í—¤ë” í–‰(ëª¨ë¸ ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ í–‰)ì„ ìœ„ì—ì„œë¶€í„° íƒìƒ‰"""
            probe = pd.read_excel(xls, sheet_name=sheet, header=None, engine="openpyxl")
            header_keywords = [r"^model$", r"^device$", r"^ì œí’ˆëª…$", r"^ëª¨ë¸$", r"^ëª¨ë¸ëª…$", r"^ê¸°ì¢…$", r"^ë‹¨ë§$", r"^ë‹¨ë§ëª…$"]
            for r in range(min(max_scan_rows, len(probe))):
                rowvals = probe.iloc[r].astype(str).fillna("")
                normvals = [_norm_hdr(v) for v in rowvals]
                if any(any(re.search(p, v) for p in header_keywords) for v in normvals):
                    return r
            return 0  # ëª» ì°¾ìœ¼ë©´ 0í–‰ ê°€ì •

        def standardize_spec_columns(df: pd.DataFrame) -> pd.DataFrame:
            """ìŠ¤í™ ì‹œíŠ¸ ì»¬ëŸ¼ ë™ì˜ì–´ë¥¼ í‘œì¤€ ì»¬ëŸ¼ìœ¼ë¡œ í†µì¼"""
            orig = list(df.columns)
            norm = [_norm_hdr(c) for c in orig]
            col_map = {}
            synonyms = {
                r"^(model|device|ì œí’ˆëª…|ì œí’ˆ|ëª¨ë¸ëª…|ëª¨ë¸|ë‹¨ë§|ë‹¨ë§ëª…|ê¸°ì¢…)$": "Model",
                r"^(maker|manufacturer|brand|oem|ì œì¡°ì‚¬|ë²¤ë”)$": "ì œì¡°ì‚¬",
                r"^(gpu|gpuëª…|gpumodel|graphics|ê·¸ë˜í”½|ê·¸ë˜í”½ì¹©|ê·¸ë˜í”½ìŠ¤|ê·¸ë˜í”½í”„ë¡œì„¸ì„œ)$": "GPU",
                r"^(chipset|soc|ap|cpu|processor)$": "Chipset",
                r"^(ram|ë©”ëª¨ë¦¬)$": "RAM",
                r"^(os|osversion|android|ios|íŒì›¨ì–´|ì†Œí”„íŠ¸ì›¨ì–´ë²„ì „|ìš´ì˜ì²´ì œ|osë²„ì „)$": "OS",
                r"^(rank|rating|ratinggrade|ë“±ê¸‰)$": "Rank",
            }
            for n, o in zip(norm, orig):
                mapped = None
                for pat, std in synonyms.items():
                    if re.search(pat, n):
                        mapped = std; break
                col_map[o] = mapped or o
            return df.rename(columns=col_map)

        # ---------- ìŠ¤í™ ì‹œíŠ¸ ì ì¬ ----------
        frames = []
        for sname in spec_sheets_selected:
            try:
                hdr = find_header_row_for_spec(xls, sname)
                dfp = pd.read_excel(xls, sheet_name=sname, header=hdr, engine="openpyxl")
            except Exception:
                continue
            dfp = standardize_spec_columns(dfp)

            # í•„ìˆ˜: Model ì—´
            model_col = "Model" if "Model" in dfp.columns else None
            if not model_col:
                for c in dfp.columns:
                    if re.search(r"^(model|device|ì œí’ˆëª…|ì œí’ˆ|ëª¨ë¸ëª…|ëª¨ë¸|ë‹¨ë§|ë‹¨ë§ëª…|ê¸°ì¢…)$", _norm_hdr(c)):
                        model_col = c; break
            if not model_col:
                continue

            # ì •ê·œí™” í‚¤ ìƒì„±
            dfp["model_norm"] = dfp[model_col].apply(normalize_model_name_strict)

            # ë³´ì¡° í‚¤(ìƒ‰ìƒÂ·ìš©ëŸ‰ ì œê±° ì „ ì›ë¬¸ë„ ë³´ê´€)
            dfp["model_raw"] = dfp[model_col].astype(str)

            # ìœ ì§€ ì»¬ëŸ¼
            keep = ["model_norm", "model_raw"] + [c for c in ["GPU","ì œì¡°ì‚¬","Chipset","RAM","OS","Rank","Model","CPU"] if c in dfp.columns]
            frames.append(dfp[keep])

        if not frames:
            st.warning("ì„ íƒí•œ ìŠ¤í™ ì‹œíŠ¸ì—ì„œ ìœ íš¨í•œ í—¤ë”/ëª¨ë¸ ì—´ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. (í—¤ë” ìœ„ì¹˜/ì—´ ì´ë¦„ í™•ì¸)")
        else:
            df_spec_all = pd.concat(frames, ignore_index=True).drop_duplicates("model_norm", keep="first")

            # ---------- ì´ìŠˆìª½ ëª¨ë¸ ì •ê·œí™” ----------
            df_final["model_norm"] = df_final["Device(Model)"].apply(normalize_model_name_strict)

            # 1ì°¨: model_normìœ¼ë¡œ ì •ì„ ë³‘í•©
            df_final = pd.merge(df_final, df_spec_all, on="model_norm", how="left", suffixes=("","_spec"))

            # Chipset ë³´ì •
            if "Chipset" not in df_final.columns and "CPU" in df_final.columns:
                df_final["Chipset"] = df_final["CPU"]

            # ì ‘ë¯¸ì‚¬ ì •ë¦¬
            for col in ["GPU","ì œì¡°ì‚¬","Chipset","RAM","OS","Rank","Model"]:
                cx, cy = f"{col}", f"{col}_spec"
                if cx in df_final.columns and cy in df_final.columns:
                    df_final[col] = df_final[cx].where(df_final[cx].notna() & (df_final[cx]!=""), df_final[cy])
                    df_final.drop(columns=[cy], inplace=True, errors="ignore")
                elif cy in df_final.columns:
                    df_final.rename(columns={cy: col}, inplace=True)

            # ---------- 2ì°¨: ë¶€ë¶„ì¼ì¹˜(contains) ë°±ì—… ë§¤ì¹­ ----------
            # ì •ì„ ë³‘í•© í›„ì—ë„ GPUê°€ ë¹„ì—ˆê³  Device(Model)ê°€ ë‚¨ì•„ìˆìœ¼ë©´, ìŠ¤í™ì˜ model_rawì— ë¶€ë¶„ í¬í•¨ë˜ëŠ”ì§€ ê²€ì‚¬
# ---------- 2ì°¨: ë¶€ë¶„ì¼ì¹˜(contains) ë°±ì—… ë§¤ì¹­ ----------

if "GPU" in df_final.columns:
    mask_need = (
        df_final["GPU"].isna()
        | (df_final["GPU"].astype(str).str.strip() == "")
    ) & (df_final["Device(Model)"].astype(str).str.len() > 0)

    if mask_need.any():
        # âœ… df_spec_allì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš©
        base_cols = ["model_raw", "GPU", "Chipset", "OS", "Rank"]
        existing_cols = [c for c in base_cols if c in df_spec_all.columns]

        if "model_raw" not in existing_cols:
            # model_raw ìì²´ê°€ ì—†ìœ¼ë©´ ë¶€ë¶„ ë§¤ì¹­ì„ í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ìŠ¤í‚µ
            st.warning("âš  ìŠ¤í™ ì‹œíŠ¸ì— model_raw(ëª¨ë¸ëª… ì›ë¬¸) ì»¬ëŸ¼ì´ ì—†ì–´ ë¶€ë¶„ ë§¤ì¹­ì„ ìƒëµí•©ë‹ˆë‹¤.")
        else:
            spec_index = (
                df_spec_all[existing_cols]
                .dropna(subset=["model_raw"])
                .reset_index(drop=True)
            )

            for idx in df_final[mask_need].index.tolist():
                dev = str(df_final.at[idx, "Device(Model)"])
                dev_norm = normalize_model_name_strict(dev)

                hit = spec_index[
                    spec_index["model_raw"]
                    .astype(str)
                    .str.replace(r"\s+", "", regex=True)
                    .str.lower()
                    .str.contains(dev_norm, regex=False)
                ]

                if hit.empty and dev_norm:
                    hit = spec_index[
                        spec_index["model_raw"]
                        .astype(str)
                        .str.lower()
                        .apply(lambda x: dev_norm in re.sub(r"\s+", "", x))
                    ]

                if not hit.empty:
                    h0 = hit.iloc[0]
                    # âœ… spec_indexì— ì‹¤ì œë¡œ ìˆëŠ” ì»¬ëŸ¼ë§Œ ì±„ìš°ë„ë¡ ë°©ì–´
                    for col in ["GPU", "Chipset", "OS", "Rank"]:
                        if col not in spec_index.columns:
                            continue
                        if (
                            col in df_final.columns
                            and (
                                pd.isna(df_final.at[idx, col])
                                or str(df_final.at[idx, col]).strip() == ""
                            )
                        ):
                            df_final.at[idx, col] = h0.get(col, "")

            # ---------- ì§„ë‹¨ ----------
            # ë§¤ì¹­ë¥ 
            if "GPU" in df_final.columns:
                matched = int(df_final["GPU"].fillna("").astype(str).str.strip().ne("").sum())
                match_rate = round(matched / max(1,len(df_final)) * 100, 1)
                st.success(f"ìŠ¤í™ ë§¤ì¹­ ê²°ê³¼: GPU ì±„ì›€ {matched} / {len(df_final)} ê±´ ({match_rate}%)")

            # ë§¤ì¹­ ì‹¤íŒ¨ ìƒ˜í”Œ ì¶œë ¥
            diag_dump("ìŠ¤í™ ë³‘í•© ë¯¸ë§¤ì¹­ ìƒ˜í”Œ(ìƒìœ„ 20)", 
                      df_final[df_final["GPU"].fillna("").astype(str).str.strip()==""][["Device(Model)","GPU","Chipset","OS","Rank"]].head(20))


# 6) ìê°€ì§„ë‹¨
with step_status("ëª¨ë“ˆ ìê°€ì§„ë‹¨"):
    diag = self_check(df_final)
    diag_dump("self_check", diag)
    if not diag["row_ok"]:
        st.error("âŒ ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ. ì¤‘ë‹¨.")
        st.stop()

# 7) ì½”ë©˜íŠ¸ ì •ê·œí™”/íƒœê¹…
with step_status("ì½”ë©˜íŠ¸ ì •ê·œí™” / íƒœê¹…"):
    def _jamo_norm(s: str) -> str:
        if s is None: return ""
        t = unicodedata.normalize("NFKC", str(s))
        t = re.sub(r"[^0-9a-zA-Zê°€-í£\s\-_+/.:]", " ", t)
        t = re.sub(r"\s+", " ", t).strip().lower()
        return t

    ISSUE_TAG_PATTERNS = [
        ("punch_hole", r"(í€ì¹˜í™€|punch[\s\-]?hole|hole[-\s]?camera)"),
        ("notch", r"(ë…¸ì¹˜|notch)"),
        ("rotation", r"(íšŒì „|ê°€ë¡œì „í™˜|ì„¸ë¡œì „í™˜|landscape|portrait|rotate)"),
        ("aspect_ratio", r"(í™”ë©´ë¹„|ë¹„ìœ¨|aspect\s?ratio)"),
        ("resolution", r"(í•´ìƒë„|resolution)"),
        ("cutout", r"(ì»·ì•„ì›ƒ|cutout)"),
        ("install", r"(ì„¤ì¹˜\s?ë¶ˆê°€|ì„¤ì¹˜ì˜¤ë¥˜|install\s?fail|íŒ¨í‚¤ì§€\s?ì˜¤ë¥˜|apk\s?ì„¤ì¹˜)"),
        ("permission", r"(ê¶Œí•œ|permission)"),
        ("login", r"(ë¡œê·¸ì¸|login|oauth|ì¸ì¦|auth)"),
        ("storage", r"(ì €ì¥ê³µê°„|storage|sd\s?card|ê¶Œí•œ\s?ê±°ë¶€)"),
        ("input_lag", r"(ì…ë ¥\s?ì§€ì—°|í„°ì¹˜\s?ì§€ì—°|ui\s?ì§€ì—°|input\s?lag|ui\s?lag)"),
        ("keyboard", r"(í‚¤ë³´ë“œ|ime|keyboard)"),
        ("ui_scaling", r"(ui\s?ìŠ¤ì¼€ì¼|í™•ëŒ€|ì¶•ì†Œ|dpi|density)"),
        ("render_artifact", r"(ì•„í‹°íŒ©íŠ¸|ê¹¨ì§|ì”ìƒ|í…Œì–´ë§|ê¸€ë¦¬ì¹˜|artifact|glitch|tearing)"),
        ("black_screen", r"(ê²€ì€\s?í™”ë©´|black\s?screen)"),
        ("white_screen", r"(í•˜ì–€\s?í™”ë©´|white\s?screen)"),
        ("crash", r"(í¬ë˜ì‹œ|fatal exception|ê°•ì œì¢…ë£Œ|crash)"),
        ("network", r"(ë„¤íŠ¸ì›Œí¬|network|ssl|handshake|timeout|unknownhost)"),
        ("audio", r"(ì†Œë¦¬|ì˜¤ë””ì˜¤|audio|ë¬´ìŒ|ë³¼ë¥¨)"),
        ("camera", r"(ì¹´ë©”ë¼|camera)"),
        ("thermal", r"(ì¨ë©€|ë°œì—´|thermal|throttl)"),
        ("fps", r"(í”„ë ˆì„|fps)"),
    ]
    def tag_issue_comment(comment: str) -> list:
        s = _jamo_norm(comment)
        tags = []
        for tag, pat in ISSUE_TAG_PATTERNS:
            if re.search(pat, s, re.I): tags.append(tag)
        return list(dict.fromkeys(tags))

    if "comment_text" not in df_final.columns: df_final["comment_text"] = ""
    df_final["comment_norm"] = df_final["comment_text"].fillna("").astype(str).apply(_jamo_norm)
    df_final["issue_tags"]   = df_final["comment_text"].fillna("").astype(str).apply(tag_issue_comment)
    diag_dump("íƒœê¹… ìƒ˜í”Œ", df_final[["Device(Model)","GPU","Chipset","OS","comment_text","issue_tags"]].head(15))

# 8) êµ°ì§‘ ì‚°ì¶œ
with step_status("êµ°ì§‘(Cluster) í†µê³„ ì‚°ì¶œ"):
    if "Chipset" not in df_final.columns and "CPU" in df_final.columns:
        df_final["Chipset"] = df_final["CPU"]

    if "GPU" not in df_final.columns: df_final["GPU"] = None
    if "Chipset" not in df_final.columns: df_final["Chipset"] = None

    df_final["GPU"] = (
        df_final["GPU"].astype(str)
        .str.replace(r"\bPower\s*VR\b", "PowerVR", regex=True)
        .str.replace(r"\bIMG\s+GE", "PowerVR GE", regex=True)
        .str.replace(r"\bGE(\d+)\b", r"PowerVR GE\1", regex=True)
    )

    def _cluster_counts(df, col, topn=15):
        if col not in df.columns: return pd.DataFrame(columns=[col,"count"])
        s = df[col].astype(object)
        s = s.where(s.notna(), "(ë¯¸ê¸°ì¬)")
        vc = s.astype(str).str.strip().replace({"nan":"(ë¯¸ê¸°ì¬)","None":"(ë¯¸ê¸°ì¬)"}).value_counts().head(topn)
        return vc.reset_index().rename(columns={"index":col, 0:"count"})

    cluster_gpu  = _cluster_counts(df_final, "GPU")
    cluster_chip = _cluster_counts(df_final, "Chipset")

    clusters = {
        "by_gpu": cluster_gpu.to_dict(orient="records"),
        "by_chipset": cluster_chip.to_dict(orient="records"),
    }

    feat_rows = []
    for idx, r in df_final.iterrows():
        for t in (r.get("issue_tags") or []):
            feat_rows.append({
                "tag": t, "row_idx": idx,
                "device": str(r.get("Device(Model)", "")),
                "gpu": str(r.get("GPU", "")),
                "chipset": str(r.get("Chipset","")),
                "os": str(r.get("OS","")),
                "comment": str(r.get("comment_text",""))
            })
    feat_df = pd.DataFrame(feat_rows)
    clusters_feature_detailed, by_issue_tag = [], []
    if not feat_df.empty:
        g = (feat_df.groupby("tag")
                .agg(count=("row_idx","size"),
                     repr_models=("device", lambda s: list(pd.Series(s).dropna().unique())[:3]),
                     evidence_rows=("row_idx", list))
                .sort_values("count", ascending=False).reset_index())
        by_issue_tag = g[["tag","count"]].rename(columns={"tag":"value"}).to_dict(orient="records")
        for _, row in g.iterrows():
            ev = []
            for ridx in row["evidence_rows"][:6]:
                rr = df_final.loc[ridx]
                ev.append({
                    "row_idx": int(ridx),
                    "device": str(rr.get("Device(Model)","")),
                    "os": str(rr.get("OS","")),
                    "comment": str(rr.get("comment_text",""))[:180]
                })
            clusters_feature_detailed.append({
                "feature_tag": row["tag"],
                "pattern": row["tag"],
                "count": int(row["count"]),
                "repr_models": row["repr_models"],
                "evidence_rows": ev,
                "singleton": (int(row["count"]) == 1)
            })

    diag_dump("GPU/Chipset êµ°ì§‘", clusters)
    diag_dump("Feature êµ°ì§‘ ìš”ì•½", by_issue_tag)
    diag_dump("Feature êµ°ì§‘ ìƒì„¸(ì¼ë¶€)", clusters_feature_detailed[:3])

# 8.5) í† í° ì ˆê°ìš© ì••ì¶• ìƒ˜í”Œ
def _compact_str(s, n=160):
    s = (str(s or "")).strip()
    return (s[:n] + "â€¦") if len(s) > n else s

def make_compact_sample(df: pd.DataFrame, per_tag=30, per_gpu=20, per_chip=20, max_rows=450):
    keep = [c for c in ["Sheet","Device(Model)","GPU","Chipset","OS","comment_text","issue_tags"] if c in df.columns]
    slim = df[keep].copy()
    slim["comment_text"] = slim["comment_text"].map(lambda x: _compact_str(x, 180))
    slim["__dedup_key__"] = (
        slim["Device(Model)"].astype(str).str.strip().str.lower()
        + "||" + slim["comment_text"].astype(str).str.strip().str.lower()
    )
    slim = slim.drop_duplicates("__dedup_key__")

    out = []
    if "issue_tags" in slim.columns:
        tag_order = ["crash","black_screen","white_screen","render_artifact","rotation",
                     "aspect_ratio","ui_scaling","resolution","permission","install",
                     "input_lag","fps","thermal","network","audio","camera","notch","punch_hole"]
        for t in tag_order:
            sub = slim[slim["issue_tags"].astype(str).str.contains(t, regex=False, na=False)].head(per_tag)
            out.append(sub)
    if "GPU" in slim.columns:
        for g in slim["GPU"].fillna("(ë¯¸ê¸°ì¬)").value_counts().head(10).index.tolist():
            out.append(slim[slim["GPU"] == g].head(per_gpu))
    if "Chipset" in slim.columns:
        for c in slim["Chipset"].fillna("(ë¯¸ê¸°ì¬)").value_counts().head(10).index.tolist():
            out.append(slim[slim["Chipset"] == c].head(per_chip))

    compact = pd.concat(out, ignore_index=True).drop_duplicates("__dedup_key__")
    compact = compact.head(max_rows).drop(columns=["__dedup_key__"], errors="ignore")
    return compact

compact_issues = make_compact_sample(df_final, per_tag=30, per_gpu=20, per_chip=20, max_rows=450)

# 9) í”„ë¡¬í”„íŠ¸ êµ¬ì„±
metrics = {
    "total_fail_issues": len(df_final),
    "clusters": clusters,
    "by_issue_tag": by_issue_tag,
    "clusters_feature_detailed": clusters_feature_detailed,
    "log_hypotheses": log_hypotheses
}
deltas, evidence_links = {}, []

sp = build_system_prompt()
up = build_user_prompt(
    project="UNKNOWN_PROJECT",
    version="UNKNOWN_VERSION",
    metrics=metrics,
    deltas=deltas,
    evidence_links=evidence_links,
    sample_issues=compact_issues,
    max_rows=500
)

# 10) OpenAI í˜¸ì¶œ (í•„ìš” ì‹œ ëª¨ë¸ë§Œ êµì²´: gpt-4o-mini)
with st.spinner("GPTê°€ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
    max_retries, wait = 3, 20
    result, last_error = None, None
    for attempt in range(max_retries):
        try:
            resp = client.chat.completions.create(
                model="gpt-5.1",          # í’ˆì§ˆ ìš°ì„  (í•„ìš” ì‹œ "gpt-4o-mini"ë¡œ ë³€ê²½)
                temperature=0.1,
                top_p=0.9,
                messages=[{"role":"system","content":sp},{"role":"user","content":up}],
                response_format={"type":"json_object"}
            )
            raw = resp.choices[0].message.content
            result = parse_llm_json(raw); result["metrics"] = metrics
            diag_dump("LLM ì›ë¬¸(ìš”ì•½)", raw[:3000])
            break
        except Exception as e:
            last_error = e
            if "429" in str(e) or "rate_limit_exceeded" in str(e).lower():
                if attempt < max_retries-1:
                    st.warning(f"429 ê°ì§€, ì¬ì‹œë„ {attempt+1}/{max_retries}")
                    time.sleep(wait); wait *= 2
                    continue
            st.error(f"OpenAI í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            st.stop()
    if result is None:
        st.error(f"OpenAI ìµœì¢… ì‹¤íŒ¨: {last_error}")
        st.stop()

# 11) ë¦¬í¬íŠ¸ ìƒì„±
try:
    output = "QA_Report.xlsx"
    write_excel_report(result, df_final, output)
    st.success("âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
    with open(output, "rb") as f:
        st.download_button("ğŸ“Š Excel ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ", f.read(), file_name=output)
except Exception as e:
    st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")



