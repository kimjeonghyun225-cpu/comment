# -*- coding: utf-8 -*-

# ìµœì¢… Streamlit ì•±: QA ê²°ê³¼ ìë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°
# - ì„¸ì…˜ ì´ˆê¸°í™” ë²„íŠ¼ ì œê³µ(í”„ë¡œì íŠ¸ëª… ì…ë ¥ ì—†ì´ ì‚¬ìš©)
# - Fail + ì½”ë©˜íŠ¸ ì¶”ì¶œ(í–‰ ê¸°ë°˜ ë””ë°”ì´ìŠ¤/OS ë™ì‹œ ì¶”ì¶œ) â†’ ë¹„ê³  ë³‘í•©(ëª¨ë“  ì„ íƒ ì‹œíŠ¸) â†’ ìŠ¤í™ ë³‘í•©
# - GPU/CPU(Chipset) êµ°ì§‘ + Feature êµ°ì§‘
# - í† í° ì ˆê°(ëŒ€í‘œ ìƒ˜í”Œ ì••ì¶•) â†’ gpt-4o(JSON ê°•ì œ) â†’ Excel ë¦¬í¬íŠ¸

import os
import re
import io
import unicodedata
import time
from contextlib import contextmanager

import pandas as pd
import openpyxl
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

from qa_patch_module import (
    find_test_sheet_candidates,
    enrich_with_column_comments,
    build_system_prompt, build_user_prompt,
    parse_llm_json, write_excel_report,
    self_check
)

# =========================
# í™˜ê²½ì„¤ì •
# =========================
load_dotenv()
api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))
if not api_key:
    st.error("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. st.secrets ë˜ëŠ” .envì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    st.stop()
client = OpenAI(api_key=api_key)

st.set_page_config(page_title="QA ê²°ê³¼ ìë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°", layout="wide")
st.title(":bar_chart: QA ê²°ê³¼ ìë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°")

# ì„¸ì…˜ ì´ˆê¸°í™” ë²„íŠ¼(í”„ë¡œì íŠ¸ ê°„ í˜¼ì… ë°©ì§€)
col_reset = st.columns([1])[0]
with col_reset:
    if st.button("ğŸ”„ ì„¸ì…˜ ì´ˆê¸°í™”"):
        st.session_state.clear()
        st.rerun()

# =========================
# ê³µí†µ ìœ í‹¸
# =========================
@contextmanager
def step_status(title: str):
    with st.status(title, expanded=False) as status:
        t0 = time.time()
        try:
            yield status
            status.update(label=f"{title} - ì™„ë£Œ ({time.time()-t0:.2f}s)", state="complete", expanded=False)
        except Exception as e:
            status.update(label=f"{title} - ì‹¤íŒ¨: {e}", state="error", expanded=True)
            raise

def diag_dump(label: str, obj):
    with st.expander(f"ğŸ” ì§„ë‹¨ ë³´ê¸°: {label}", expanded=False):
        st.write(obj)

def _norm(s: str) -> str:
    s = unicodedata.normalize("NFKC", str(s))
    s = re.sub(r"[\s\-\_/()\[\]{}:+Â·âˆ™â€¢]", "", s)
    return s.lower().strip()

def normalize_model_name_strict(s):
    if pd.isna(s): return ""
    s = str(s)
    s = re.sub(r"\(.*?\)", "", s)
    s = re.sub(r"\b(64|128|256|512)\s*gb\b", "", s, flags=re.I)
    s = re.sub(r"\b(black|white|blue|red|green|gold|silver|ê³¨ë“œ|ë¸”ë™|í™”ì´íŠ¸|ì‹¤ë²„)\b", "", s, flags=re.I)
    s = re.sub(r"[\s\-_]+", "", s)
    return s.lower().strip()

# =========================
# í—¤ë” íƒì§€: í–‰ ê¸°ë°˜ ë””ë°”ì´ìŠ¤/OS ì¶”ì¶œìš©
# =========================
def _detect_header_map(ws, max_scan_rows=40):
    """
    ì›Œí¬ì‹œíŠ¸ ìƒë‹¨ì—ì„œ í—¤ë” í–‰ì„ ì°¾ê³ , ë””ë°”ì´ìŠ¤/OS ê´€ë ¨ ì¹¼ëŸ¼ ì¸ë±ìŠ¤ë¥¼ ë§¤í•‘.
    ë°˜í™˜: {"header_row": r, "device_col": c or None, "os_col": c or None}
    """
    def _norm_cell(v):
        s = unicodedata.normalize("NFKC", str(v or "")).lower().strip()
        s = re.sub(r"[\s\-\_/()\[\]{}:+Â·âˆ™â€¢]", "", s)  # ê³µë°±/êµ¬ë¶„ì ì œê±°
        return s

    # í­ë„“ì€ íŒ¨í„´(ì •í™•ì¼ì¹˜ + ë¶€ë¶„ì¼ì¹˜)
    dev_exact = re.compile(r"^(device|model|ëª¨ë¸|ëª¨ë¸ëª…|ì œí’ˆ|ì œí’ˆëª…|ë‹¨ë§|ë‹¨ë§ê¸°ëª…)$")
    dev_contains = re.compile(r"(device|model|ëª¨ë¸|ëª¨ë¸ëª…|ì œí’ˆ|ì œí’ˆëª…|ë‹¨ë§)")
    os_exact  = re.compile(r"^(os|osversion|android|ios|íŒì›¨ì–´|ì†Œí”„íŠ¸ì›¨ì–´ë²„ì „)$")
    os_contains = re.compile(r"(os|android|ios|íŒì›¨ì–´|ì†Œí”„íŠ¸ì›¨ì–´)")

    header_row, device_col, os_col = None, None, None
    max_c = ws.max_column

    for r in range(1, min(max_scan_rows, ws.max_row) + 1):
        values = [ws.cell(row=r, column=c).value for c in range(1, max_c + 1)]
        normed = [_norm_cell(v) for v in values]
        if sum(1 for v in normed if v) < 2:
            continue

        d_idx, o_idx = None, None
        for ci, v in enumerate(normed, start=1):
            if not v:
                continue
            # ë””ë°”ì´ìŠ¤ ì»¬ëŸ¼ í›„ë³´
            if dev_exact.match(v) or dev_contains.search(v) or v in ("devicemodel", "devicemodelname", "devicename"):
                if d_idx is None:
                    d_idx = ci
            # OS ì»¬ëŸ¼ í›„ë³´
            if os_exact.match(v) or os_contains.search(v):
                if o_idx is None:
                    o_idx = ci

        if d_idx or o_idx:
            header_row, device_col, os_col = r, d_idx, o_idx
            break

    return {"header_row": header_row, "device_col": device_col, "os_col": os_col}

# =========================
# íŒŒì¼ ì—…ë¡œë“œ
# =========================
st.caption("â€» Logcat ë¶„ì„ì€ í˜„ì¬ ë¹„í™œì„±í™” ìƒíƒœì…ë‹ˆë‹¤.")
uploaded_file = st.file_uploader("ì›ë³¸ QA ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"])

if uploaded_file:
    # íŒŒì¼ í¬ì¸í„° ê³ ì •
    data = uploaded_file.read()

    with step_status("ì—‘ì…€ ë¡œë“œ"):
        xls = pd.ExcelFile(io.BytesIO(data), engine="openpyxl")
        diag_dump("ì‹œíŠ¸ ëª©ë¡", xls.sheet_names)

    # 1) í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ìë™ê°ì§€
    with step_status("í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ìë™ê°ì§€"):
        test_candidates = find_test_sheet_candidates(xls)
        diag_dump("ê°ì§€ëœ í›„ë³´ ì‹œíŠ¸", test_candidates)

    st.subheader("1. í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ì„ íƒ")
    test_sheets_selected = st.multiselect(
        "ìë™ ê°ì§€ëœ í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ì¤‘ ë¶„ì„ ëŒ€ìƒ ì„ íƒ",
        options=test_candidates,
        default=test_candidates[:2]
    )
    if not test_sheets_selected:
        st.error("âŒ ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
        st.stop()

    # 2) ìŠ¤í™ ì‹œíŠ¸ ì„ íƒ
    st.subheader("2. ìŠ¤í™ ì‹œíŠ¸ ì„ íƒ (ë””ë°”ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸)")
    default_spec = [s for s in ["AOS_Device_List", "iOS_Device_List"] if s in xls.sheet_names]
    spec_sheets_selected = st.multiselect(
        "ìŠ¤í™(Chipset, GPU, OS, Rank ë“±) í¬í•¨ ì‹œíŠ¸ ì„ íƒ",
        options=xls.sheet_names,
        default=default_spec
    )
    st.markdown("---")

    # ì‹¤í–‰
    if st.button("ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±", type="primary"):
        log_hypotheses = []
        clusters = {}
        evidence_links = []

        # 3) Fail + ì…€ ì½”ë©˜íŠ¸ ì¶”ì¶œ (í–‰ ê¸°ë°˜ ë””ë°”ì´ìŠ¤/OS ë™ì‹œ ì¶”ì¶œ)
        with step_status("Fail + ì…€ ì½”ë©˜íŠ¸ ì¶”ì¶œ"):
            wb = openpyxl.load_workbook(io.BytesIO(data), data_only=True)
            df_issue = []

            for s in test_sheets_selected:
                ws = wb[s]
                hdr = _detect_header_map(ws, max_scan_rows=12)
                header_row = hdr.get("header_row")
                dev_col = hdr.get("device_col")
                os_col  = hdr.get("os_col")

                for row in ws.iter_rows():
                    for cell in row:
                        if isinstance(cell.value, str) and cell.value.strip().lower() == "fail" and cell.comment:
                            dev_val = ""
                            os_val = ""
                            if header_row and cell.row > header_row:
                                if dev_col:
                                    dev_val = ws.cell(row=cell.row, column=dev_col).value
                                if os_col:
                                    os_val = ws.cell(row=cell.row, column=os_col).value

                            df_issue.append({
                                "Sheet": s,
                                "Checklist": ws.title,
                                "Device(Model)": str(dev_val or "").strip(),
                                "OS": str(os_val or "").strip(),
                                "comment_cell": (cell.comment.text or "").strip()
                            })

            df_issue = pd.DataFrame(df_issue) if df_issue else pd.DataFrame(
                columns=["Sheet", "Checklist", "Device(Model)", "OS", "comment_cell"]
            )
            if df_issue.empty:
                st.warning("âŒ Fail+ì½”ë©˜íŠ¸ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤(ì…€ ì½”ë©˜íŠ¸ ê¸°ì¤€). ë¹„ê³ /Notesë§Œìœ¼ë¡œë„ êµ°ì§‘í™”í•˜ë ¤ë©´ ì›ë³¸ ì‹œíŠ¸ì˜ ë¹„ê³ ì—´ì„ í™œìš©í•˜ì‹­ì‹œì˜¤.")
                st.stop()

        # 4) ë¹„ê³ /Notes ë³‘í•© (ëª¨ë“  ì„ íƒ ì‹œíŠ¸ì— ëŒ€í•´ ë³‘í•©)
        with step_status("ë¹„ê³ /Notes ë³‘í•©"):
            for _sheet in test_sheets_selected:
                df_issue = enrich_with_column_comments(xls, _sheet, df_issue)
            diag_dump("ë³‘í•© ê²°ê³¼ ìƒ˜í”Œ", df_issue.head(10))

        # 5) ìŠ¤í™ ë³‘í•©
        df_final = df_issue.copy()
        match_rate = 0.0
        if spec_sheets_selected:
            with step_status("ìŠ¤í™ ë³‘í•©"):
                try:
                    def _norm_for_header(s: str) -> str:
                        s = unicodedata.normalize("NFKC", str(s))
                        s = re.sub(r"[\s\-\_/()\[\]{}:+Â·âˆ™â€¢]", "", s)
                        return s.lower().strip()

                    def find_header_row_for_spec(xls, sheet, max_scan_rows=12):
                        df_probe = pd.read_excel(xls, sheet_name=sheet, header=None, engine="openpyxl")
                        header_row_idx = 0
                        header_candidates = [r"^model$", r"^device$", r"^ì œí’ˆëª…$", r"^ì œí’ˆ$", r"^ëª¨ë¸ëª…$", r"^ëª¨ë¸$"]
                        for r in range(min(max_scan_rows, len(df_probe))):
                            row_vals = df_probe.iloc[r].astype(str).fillna("")
                            norm_vals = [_norm_for_header(v) for v in row_vals]
                            for v in norm_vals:
                                if any(re.search(pat, v) for pat in header_candidates):
                                    header_row_idx = r; break
                            if header_row_idx: break
                        return header_row_idx

                    def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
                        original_cols = list(df.columns)
                        norm_cols = [_norm_for_header(c) for c in original_cols]
                        col_map = {}
                        synonyms = {
                            r"^(model|device|ì œí’ˆëª…|ì œí’ˆ|ëª¨ë¸ëª…|ëª¨ë¸)$": "Model",
                            r"^(maker|manufacturer|brand|oem|ì œì¡°ì‚¬|ë²¤ë”)$": "ì œì¡°ì‚¬",
                            r"^(gpu|ê·¸ë˜í”½|ê·¸ë˜í”½ì¹©|ê·¸ë˜í”½ìŠ¤|ê·¸ë˜í”½í”„ë¡œì„¸ì„œ)$": "GPU",
                            r"^(chipset|soc|ap|cpu|processor)$": "Chipset",  # processor ì¶”ê°€
                            r"^(ram|ë©”ëª¨ë¦¬)$": "RAM",
                            r"^(os|osversion|android|ios|íŒì›¨ì–´|ì†Œí”„íŠ¸ì›¨ì–´ë²„ì „)$": "OS",
                            r"^(rank|rating|ratinggrade|ë“±ê¸‰)$": "Rank",
                        }
                        for norm_name, orig_name in zip(norm_cols, original_cols):
                            mapped = None
                            for pat, std_name in synonyms.items():
                                if re.search(pat, norm_name):
                                    mapped = std_name; break
                            col_map[orig_name] = mapped or orig_name
                        return df.rename(columns=col_map)

                    def detect_model_col(df: pd.DataFrame):
                        if "Model" in df.columns:
                            return "Model"
                        for c in df.columns:
                            n = _norm_for_header(c)
                            if re.search(r"^(model|device|ì œí’ˆëª…|ì œí’ˆ|ëª¨ë¸ëª…|ëª¨ë¸)$", n):
                                return c
                        return None

                    def load_std_spec_df(xls, sheet):
                        hdr = find_header_row_for_spec(xls, sheet)
                        df = pd.read_excel(xls, sheet_name=sheet, header=hdr, engine="openpyxl")
                        df = standardize_columns(df)
                        model_col = detect_model_col(df)
                        if model_col is None:
                            raise ValueError(f"'{sheet}'ì—ì„œ ëª¨ë¸ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {list(df.columns)}")
                        df["model_norm"] = df[model_col].apply(normalize_model_name_strict)
                        cols_keep = ["model_norm"]
                        for c in ["GPU", "ì œì¡°ì‚¬", "Chipset", "RAM", "OS", "Rank", "Model", "CPU"]:
                            if c in df.columns: cols_keep.append(c)
                        return df[cols_keep]

                    spec_frames = [load_std_spec_df(xls, s) for s in spec_sheets_selected]
                    df_spec_all = pd.concat(spec_frames, ignore_index=True)
                    df_spec_all = df_spec_all.drop_duplicates(subset=["model_norm"], keep="first")

                    df_final["model_norm"] = df_final["Device(Model)"].apply(normalize_model_name_strict)
                    df_final = pd.merge(df_final, df_spec_all, on="model_norm", how="left")

                    # CPU â†’ Chipset í´ë°±
                    if "Chipset" not in df_final.columns and "CPU" in df_final.columns:
                        df_final["Chipset"] = df_final["CPU"]

                    # ì ‘ë¯¸ì‚¬ ì •ë¦¬
                    merge_cols = ["GPU", "ì œì¡°ì‚¬", "Chipset", "RAM", "OS", "Rank", "Model"]
                    for col in merge_cols:
                        cx, cy = f"{col}_x", f"{col}_y"
                        if cx in df_final.columns and cy in df_final.columns:
                            df_final[col] = df_final[cx].where(df_final[cx].notna(), df_final[cy])
                            df_final.drop(columns=[cx, cy], inplace=True)
                        elif cx in df_final.columns:
                            df_final.rename(columns={cx: col}, inplace=True)
                        elif cy in df_final.columns:
                            df_final.rename(columns={cy: col}, inplace=True)

                    if "GPU" in df_final.columns:
                        matched = int(df_final["GPU"].notna().sum())
                        match_rate = round(matched / len(df_final) * 100, 1)
                        st.success(f"ìŠ¤í™ ë§¤ì¹­ ê²°ê³¼: {matched} / {len(df_final)} ê±´ ({match_rate}%)")
                except Exception as e:
                    st.error(f"ìŠ¤í™ ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}")

        # 6) ìê°€ì§„ë‹¨
        with step_status("ëª¨ë“ˆ ìê°€ì§„ë‹¨"):
            diag = self_check(df_final)
            diag_dump("self_check ê²°ê³¼", diag)
            if not diag["row_ok"]:
                st.error("âŒ ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ. ì¤‘ë‹¨.")
                st.stop()

        # 7) ì½”ë©˜íŠ¸ ì •ê·œí™” / íƒœê¹…
        with step_status("ì½”ë©˜íŠ¸ ì •ê·œí™” / íƒœê¹…"):
            def _jamo_norm(s: str) -> str:
                if s is None: return ""
                t = unicodedata.normalize("NFKC", str(s))
                t = re.sub(r"[^0-9a-zA-Zê°€-í£\s\-_+/.:]", " ", t)
                t = re.sub(r"\s+", " ", t).strip().lower()
                return t

            ISSUE_TAG_PATTERNS = [
                ("punch_hole",   r"(í€ì¹˜í™€|punch[\s\-]?hole|hole[-\s]?camera)"),
                ("notch",        r"(ë…¸ì¹˜|notch)"),
                ("rotation",     r"(íšŒì „|ê°€ë¡œì „í™˜|ì„¸ë¡œì „í™˜|landscape|portrait|rotate)"),
                ("aspect_ratio", r"(í™”ë©´ë¹„|ë¹„ìœ¨|aspect\s?ratio)"),
                ("resolution",   r"(í•´ìƒë„|resolution)"),
                ("cutout",       r"(ì»·ì•„ì›ƒ|cutout)"),
                ("install",      r"(ì„¤ì¹˜\s?ë¶ˆê°€|ì„¤ì¹˜ì˜¤ë¥˜|install\s?fail|íŒ¨í‚¤ì§€\s?ì˜¤ë¥˜|apk\s?ì„¤ì¹˜)"),
                ("permission",   r"(ê¶Œí•œ|permission)"),
                ("login",        r"(ë¡œê·¸ì¸|login|oauth|ì¸ì¦|auth)"),
                ("storage",      r"(ì €ì¥ê³µê°„|storage|sd\s?card|ê¶Œí•œ\s?ê±°ë¶€)"),
                ("input_lag",    r"(ì…ë ¥\s?ì§€ì—°|ì§€ì—°\s?ì…ë ¥|í„°ì¹˜\s?ì§€ì—°|ui\s?ì§€ì—°|input\s?lag|ui\s?lag)"),
                ("keyboard",     r"(í‚¤ë³´ë“œ|ime|keyboard)"),
                ("ui_scaling",   r"(ui\s?ìŠ¤ì¼€ì¼|í™•ëŒ€|ì¶•ì†Œ|dpi|density)"),
                ("render_artifact", r"(ì•„í‹°íŒ©íŠ¸|ê¹¨ì§|ì”ìƒ|í…Œì–´ë§|ê¸€ë¦¬ì¹˜|artifact|glitch|tearing)"),
                ("black_screen", r"(ê²€ì€\s?í™”ë©´|black\s?screen)"),
                ("white_screen", r"(í•˜ì–€\s?í™”ë©´|white\s?screen)"),
                ("crash",        r"(í¬ë˜ì‹œ|fatal exception|ê°•ì œì¢…ë£Œ|crash)"),
                ("network",      r"(ë„¤íŠ¸ì›Œí¬|network|ssl|handshake|timeout|unknownhost)"),
                ("audio",        r"(ì†Œë¦¬|ì˜¤ë””ì˜¤|audio|ë¬´ìŒ|ë³¼ë¥¨)"),
                ("camera",       r"(ì¹´ë©”ë¼|camera)"),
                ("thermal",      r"(ì¨ë©€|ë°œì—´|thermal|throttl)"),
                ("fps",          r"(í”„ë ˆì„|fps)"),
            ]

            def tag_issue_comment(comment: str) -> list:
                s = _jamo_norm(comment)
                tags = []
                for tag, pat in ISSUE_TAG_PATTERNS:
                    if re.search(pat, s, re.I):
                        tags.append(tag)
                return list(dict.fromkeys(tags))

            if "comment_text" not in df_final.columns:
                df_final["comment_text"] = ""
            df_final["comment_norm"] = df_final["comment_text"].fillna("").astype(str).apply(_jamo_norm)
            df_final["issue_tags"]   = df_final["comment_text"].fillna("").astype(str).apply(tag_issue_comment)
            diag_dump("íƒœê¹… ìƒ˜í”Œ", df_final[["Device(Model)","GPU","Chipset","OS","comment_text","issue_tags"]].head(15))

        # 8) êµ°ì§‘(Cluster) í†µê³„ ì‚°ì¶œ
        with step_status("êµ°ì§‘(Cluster) í†µê³„ ì‚°ì¶œ"):
            # Chipset í´ë°± ì‚¬ìš©
            if "Chipset" not in df_final.columns and "CPU" in df_final.columns:
                df_final["Chipset"] = df_final["CPU"]

            # ì§„ë‹¨: ìŠ¤í™ ì±„ì›€ ë¹„ìœ¨
            gpu_fill = int(df_final.get("GPU", pd.Series([None]*len(df_final))).notna().sum()) if "GPU" in df_final.columns else 0
            chip_fill = int(df_final.get("Chipset", pd.Series([None]*len(df_final))).notna().sum()) if "Chipset" in df_final.columns else 0
            st.caption(f"ğŸ” GPU ì±„ì›€: {gpu_fill}/{len(df_final)} Â· Chipset ì±„ì›€: {chip_fill}/{len(df_final)}")

            if "GPU" not in df_final.columns:
                df_final["GPU"] = None
            if "Chipset" not in df_final.columns:
                df_final["Chipset"] = None

            def _cluster_counts(df, col, topn=15):
                if col not in df.columns:
                    return pd.DataFrame(columns=[col, "count"])
                vc = df[col].fillna("(ë¯¸ê¸°ì¬)").astype(str).str.strip().value_counts().head(topn)
                return vc.reset_index().rename(columns={"index": col, 0: "count"})

            if "GPU" in df_final.columns:
                df_final["GPU"] = (
                    df_final["GPU"].astype(str)
                    .str.replace(r"\bPower\s*VR\b", "PowerVR", regex=True)
                    .str.replace(r"\bIMG\s+GE", "PowerVR GE", regex=True)
                    .str.replace(r"\bGE(\d+)\b", r"PowerVR GE\1", regex=True)
                )

            cluster_gpu = _cluster_counts(df_final, "GPU")
            cluster_chip = _cluster_counts(df_final, "Chipset")

            clusters = {
                "by_gpu": cluster_gpu.to_dict(orient="records"),
                "by_chipset": cluster_chip.to_dict(orient="records"),
            }

            # Feature êµ°ì§‘(íƒœê·¸ ê¸°ë°˜)
            feat_rows = []
            for idx, r in df_final.iterrows():
                for t in (r.get("issue_tags") or []):
                    feat_rows.append({
                        "tag": t,
                        "row_idx": idx,
                        "device": str(r.get("Device(Model)", "")),
                        "gpu": str(r.get("GPU", "")),
                        "chipset": str(r.get("Chipset", "")),
                        "os": str(r.get("OS","")),
                        "comment": str(r.get("comment_text",""))
                    })
            feat_df = pd.DataFrame(feat_rows)
            clusters_feature_detailed = []
            by_issue_tag = []
            if not feat_df.empty:
                g = (feat_df.groupby("tag")
                            .agg(count=("row_idx","size"),
                                 repr_models=("device", lambda s: list(pd.Series(s).dropna().unique())[:3]),
                                 evidence_rows=("row_idx", list))
                            .sort_values("count", ascending=False)
                            .reset_index())
                by_issue_tag = g[["tag","count"]].rename(columns={"tag":"value"}).to_dict(orient="records")

                def _row_evidence(r):
                    return {
                        "row_idx": int(getattr(r, "name", -1)),
                        "device": str(r.get("Device(Model)", "")),
                        "os": str(r.get("OS", "")),
                        "comment": str(r.get("comment_text", ""))[:180]
                    }

                for _, row in g.iterrows():
                    ev = []
                    for ridx in row["evidence_rows"][:6]:
                        rr = df_final.loc[ridx]
                        ev.append(_row_evidence(rr))
                    clusters_feature_detailed.append({
                        "feature_tag": row["tag"],
                        "pattern": row["tag"],
                        "count": int(row["count"]),
                        "repr_models": row["repr_models"],
                        "evidence_rows": ev,
                        "singleton": (int(row["count"]) == 1)
                    })

            diag_dump("GPU/Chipset êµ°ì§‘", clusters)
            diag_dump("Feature êµ°ì§‘ ìš”ì•½", by_issue_tag)
            diag_dump("Feature êµ°ì§‘ ìƒì„¸(ì¼ë¶€)", clusters_feature_detailed[:3])

            if cluster_gpu.empty and cluster_chip.empty:
                st.warning("GPU/Chipset êµ°ì§‘ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ìŠ¤í™ ë³‘í•© ì‹¤íŒ¨ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. "
                           "â†’ í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ì—ì„œ ë””ë°”ì´ìŠ¤ëª…ì´ ì œëŒ€ë¡œ ì±„ì›Œì§€ëŠ”ì§€ì™€, ìŠ¤í™ ì‹œíŠ¸ ì¹¼ëŸ¼ëª…ì´ Model/Chipset/GPUë¡œ í‘œì¤€í™”ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")

        # 8.5) gpt-4o í† í° ì ˆê°: ëŒ€í‘œ ìƒ˜í”Œë§Œ ì••ì¶• ì¶”ì¶œ
        def _compact_str(s, n=160):
            s = (str(s or "")).strip()
            return (s[:n] + "â€¦") if len(s) > n else s

        def make_compact_sample(df: pd.DataFrame, per_tag=30, per_gpu=20, per_chip=20, max_rows=450):
            keep = [c for c in ["Sheet","Device(Model)","GPU","Chipset","OS","comment_text","issue_tags"] if c in df.columns]
            slim = df[keep].copy()
            slim["comment_text"] = slim["comment_text"].map(lambda x: _compact_str(x, 180))
            slim["__dedup_key__"] = (
                slim["Device(Model)"].astype(str).str.strip().str.lower()
                + "||" + slim["comment_text"].astype(str).str.strip().str.lower()
            )
            slim = slim.drop_duplicates("__dedup_key__")

            out = []
            if "issue_tags" in slim.columns:
                tag_order = ["crash","black_screen","white_screen","render_artifact","rotation",
                             "aspect_ratio","ui_scaling","resolution","permission","install",
                             "input_lag","fps","thermal","network","audio","camera","notch","punch_hole"]
                for t in tag_order:
                    sub = slim[slim["issue_tags"].astype(str).str.contains(t, regex=False, na=False)].head(per_tag)
                    out.append(sub)
            if "GPU" in slim.columns:
                for g in slim["GPU"].fillna("(ë¯¸ê¸°ì¬)").value_counts().head(10).index.tolist():
                    out.append(slim[slim["GPU"] == g].head(per_gpu))
            if "Chipset" in slim.columns:
                for c in slim["Chipset"].fillna("(ë¯¸ê¸°ì¬)").value_counts().head(10).index.tolist():
                    out.append(slim[slim["Chipset"] == c].head(per_chip))

            compact = pd.concat(out, ignore_index=True).drop_duplicates("__dedup_key__")
            compact = compact.head(max_rows).drop(columns=["__dedup_key__"], errors="ignore")
            return compact

        compact_issues = make_compact_sample(df_final, per_tag=30, per_gpu=20, per_chip=20, max_rows=450)

        # 9) í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ + í† í° ì˜ˆì‚° ì¡°ì •
        metrics = {
            "total_fail_issues": len(df_final),
            "clusters": clusters,
            "by_issue_tag": by_issue_tag,
            "clusters_feature_detailed": clusters_feature_detailed,
            "log_hypotheses": log_hypotheses
        }
        deltas, evidence_links = {}, []

        def _rough_token_count(t: str) -> int:
            return max(1, int(len(t) / 2.5))

        def estimate_tokens(msgs: list) -> int:
            try:
                import tiktoken
                enc = tiktoken.get_encoding("cl100k_base")
                return sum(len(enc.encode(m.get("content",""))) for m in msgs)
            except Exception:
                return sum(_rough_token_count(m.get("content","")) for m in msgs)

        def fit_prompt(build_user, base_kwargs, model_budget=30000, reserve_output=6000):
            max_rows_list = [800, 600, 400, 300, 200, 100]
            df = base_kwargs["sample_issues"]
            for mr in max_rows_list:
                kwargs = dict(base_kwargs)
                kwargs["sample_issues"] = df.head(mr)
                sp = build_system_prompt()
                up = build_user(**kwargs)
                used = estimate_tokens([{"content": sp},{"content": up}])
                if used + reserve_output < model_budget:
                    return sp, up, {"prompt_tokens_est": used, "max_rows": mr}
            return sp, up, {"warn": "budget_exceeded"}

        base_kwargs = {
            "project": "UNKNOWN_PROJECT",
            "version": "UNKNOWN_VERSION",
            "metrics": metrics,
            "deltas": deltas,
            "evidence_links": evidence_links,
            "sample_issues": compact_issues,  # í•µì‹¬: ì••ì¶•ë³¸ ì‚¬ìš©
            "max_rows": 500
        }
        with step_status("í† í° ì˜ˆì‚° ì¡°ì •"):
            sp, up, diag_budget = fit_prompt(build_user_prompt, base_kwargs)
            diag_dump("í† í° ì§„ë‹¨", diag_budget)

        # 10) OpenAI í˜¸ì¶œ
        with st.spinner("GPTê°€ ë¦¬í¬íŠ¸ë¥¼ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤... (429 ì˜¤ë¥˜ ì‹œ ìë™ ì¬ì‹œë„)"):
            max_retries = 3
            wait_time_seconds = 20
            last_error = None
            result = None

            for attempt in range(max_retries):
                try:
                    resp = client.chat.completions.create(
                        model="gpt-4o",
                        temperature=0.1,
                        top_p=0.9,
                        messages=[{"role":"system","content":sp},{"role":"user","content":up}],
                        response_format={"type": "json_object"}  # JSON ëª¨ë“œ ê°•ì œ
                    )
                    raw = resp.choices[0].message.content
                    result = parse_llm_json(raw)
                    result["metrics"] = metrics
                    diag_dump("LLM ì›ë¬¸(ìš”ì•½)", raw[:4000])
                    last_error = None
                    break

                except Exception as e:
                    last_error = e
                    error_message = str(e).lower()

                    if "rate_limit_exceeded" in error_message or "429" in error_message:
                        if attempt < max_retries - 1:
                            st.warning(f"â³ RATE LIMIT (429) ê°ì§€ (ì‹œë„ {attempt + 1}/{max_retries}). {wait_time_seconds}ì´ˆ í›„ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                            time.sleep(wait_time_seconds)
                            wait_time_seconds *= 2
                        else:
                            st.error(f"âŒ RATE LIMIT (429) ì˜¤ë¥˜. ì¬ì‹œë„({max_retries}íšŒ) ëª¨ë‘ ì‹¤íŒ¨.")
                            st.stop()
                    else:
                        st.error(f"âŒ OpenAI í˜¸ì¶œ ì¤‘ ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        st.stop()

            if result is None:
                st.error(f"âŒ OpenAI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {last_error}")
                st.stop()

        # 11) ì—‘ì…€ ë¦¬í¬íŠ¸ ìƒì„±
        try:
            output = "QA_Report.xlsx"
            write_excel_report(result, df_final, output)
            st.success("âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            with open(output, "rb") as f:
                st.download_button("ğŸ“Š Excel ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ", f.read(), file_name=output)
        except Exception as e:
            st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")

