# -*- coding: utf-8 -*-
# ìµœì¢… Streamlit ì•±: QA ê²°ê³¼ ìžë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°
# - ì„¸ì…˜ ì´ˆê¸°í™” ë²„íŠ¼ ì œê³µ(í”„ë¡œì íŠ¸ëª… ìž…ë ¥ ì—†ì´ ì‚¬ìš©)
# - Fail + ì½”ë©˜íŠ¸ ì¶”ì¶œ â†’ ë¹„ê³  ë³‘í•© â†’ ìŠ¤íŽ™ ë³‘í•© â†’ GPU/CPU êµ°ì§‘ + Feature(íŽ€ì¹˜í™€/ë…¸ì¹˜/íšŒì „/ì„¤ì¹˜/ê¶Œí•œ/ìž…ë ¥ì§€ì—° ë“±) êµ°ì§‘
# - í† í° ì˜ˆì‚° ìžë™ ì¡°ì • â†’ LLM(JSON ê°•ì œ) â†’ Excel ë¦¬í¬íŠ¸

import os
import re
import io
import zipfile
import unicodedata
import time
from contextlib import contextmanager

import pandas as pd
import openpyxl
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

from qa_patch_module import (
    find_test_sheet_candidates,
    enrich_with_column_comments,
    build_system_prompt, build_user_prompt,
    parse_llm_json, write_excel_report,
    self_check
)

# =========================
# í™˜ê²½ì„¤ì •
# =========================
load_dotenv()
api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))
if not api_key:
    st.error("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. st.secrets ë˜ëŠ” .envì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    st.stop()
client = OpenAI(api_key=api_key)

st.set_page_config(page_title="QA ê²°ê³¼ ìžë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°", layout="wide")
st.title(":bar_chart: QA ê²°ê³¼ ìžë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°")

# ì„¸ì…˜ ì´ˆê¸°í™” ë²„íŠ¼(í”„ë¡œì íŠ¸ ê°„ í˜¼ìž… ë°©ì§€)
col_reset = st.columns([1])[0]
with col_reset:
    if st.button("ðŸ”„ ì„¸ì…˜ ì´ˆê¸°í™”"):
        st.session_state.clear()
        st.rerun()  # experimental_rerun â†’ rerun

# =========================
# ê³µí†µ ìœ í‹¸
# =========================
@contextmanager
def step_status(title: str):
    with st.status(title, expanded=False) as status:
        t0 = time.time()
        try:
            yield status
            status.update(label=f"{title} - ì™„ë£Œ ({time.time()-t0:.2f}s)", state="complete", expanded=False)
        except Exception as e:
            status.update(label=f"{title} - ì‹¤íŒ¨: {e}", state="error", expanded=True)
            raise

def diag_dump(label: str, obj):
    with st.expander(f"ðŸ”Ž ì§„ë‹¨ ë³´ê¸°: {label}", expanded=False):
        st.write(obj)

def _norm(s: str) -> str:
    s = unicodedata.normalize("NFKC", str(s))
    s = re.sub(r"[\s\-\_/()\[\]{}:+Â·âˆ™â€¢]", "", s)
    return s.lower().strip()

def normalize_model_name_strict(s):
    if pd.isna(s): return ""
    s = str(s)
    s = re.sub(r"\(.*?\)", "", s)
    s = re.sub(r"\b(64|128|256|512)\s*gb\b", "", s, flags=re.I)
    s = re.sub(r"\b(black|white|blue|red|green|gold|silver|ê³¨ë“œ|ë¸”ëž™|í™”ì´íŠ¸|ì‹¤ë²„)\b", "", s, flags=re.I)
    s = re.sub(r"[\s\-_]+", "", s)
    return s.lower().strip()

# =========================
# Logcat(ë¹„í™œì„±)
# =========================
log_files = None
st.caption("â€» Logcat ë¶„ì„ì€ í˜„ìž¬ ë¹„í™œì„±í™” ìƒíƒœìž…ë‹ˆë‹¤.")

# =========================
# íŒŒì¼ ì—…ë¡œë“œ
# =========================
uploaded_file = st.file_uploader("ì›ë³¸ QA ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"])

if uploaded_file:
    with step_status("ì—‘ì…€ ë¡œë“œ"):
        xls = pd.ExcelFile(uploaded_file, engine="openpyxl")
        diag_dump("ì‹œíŠ¸ ëª©ë¡", xls.sheet_names)

    # 1) í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ìžë™ê°ì§€
    with step_status("í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ìžë™ê°ì§€"):
        test_candidates = find_test_sheet_candidates(xls)
        diag_dump("ê°ì§€ëœ í›„ë³´ ì‹œíŠ¸", test_candidates)

    st.subheader("1. í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ì„ íƒ")
    test_sheets_selected = st.multiselect(
        "ìžë™ ê°ì§€ëœ í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ì¤‘ ë¶„ì„ ëŒ€ìƒ ì„ íƒ",
        options=test_candidates,
        default=test_candidates[:2]
    )
    if not test_sheets_selected:
        st.error("âŒ ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
        st.stop()

    # 2) ìŠ¤íŽ™ ì‹œíŠ¸ ì„ íƒ
    st.subheader("2. ìŠ¤íŽ™ ì‹œíŠ¸ ì„ íƒ (ë””ë°”ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸)")
    default_spec = [s for s in ["AOS_Device_List", "iOS_Device_List"] if s in xls.sheet_names]
    spec_sheets_selected = st.multiselect(
        "ìŠ¤íŽ™(Chipset, GPU, OS, Rank ë“±) í¬í•¨ ì‹œíŠ¸ ì„ íƒ",
        options=xls.sheet_names,
        default=default_spec
    )
    st.markdown("---")

    # ì‹¤í–‰
    if st.button("ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±", type="primary"):
        # ì‹¤í–‰ë³„ ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
        log_summary = {}
        log_hypotheses = []
        clusters = {}
        evidence_links = []

        # 3) Fail + ì…€ ì½”ë©˜íŠ¸ ì¶”ì¶œ (ì…€ ì½”ë©˜íŠ¸ê°€ ì—†ìœ¼ë©´ ë¹„ê³  í†µí•©ë§Œìœ¼ë¡œ ì§„í–‰)
        with step_status("Fail + ì…€ ì½”ë©˜íŠ¸ ì¶”ì¶œ"):
            wb = openpyxl.load_workbook(uploaded_file, data_only=True)
            df_issue = []
            for s in test_sheets_selected:
                ws = wb[s]
                for row in ws.iter_rows():
                    for cell in row:
                        if isinstance(cell.value, str) and cell.value.lower() == "fail" and cell.comment:
                            df_issue.append({
                                "Sheet": s,
                                "Checklist": ws.title,
                                "Device(Model)": "",  # ìŠ¤íŽ™ ë³‘í•© í›„ ì±„ì›Œì§ˆ ìˆ˜ ìžˆìŒ
                                "comment_cell": (cell.comment.text or "").strip()
                            })
            df_issue = pd.DataFrame(df_issue) if df_issue else pd.DataFrame(columns=["Sheet","Checklist","Device(Model)","comment_cell"])
            if df_issue.empty:
                st.warning("âŒ Fail+ì½”ë©˜íŠ¸ í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤(ì…€ ì½”ë©˜íŠ¸ ê¸°ì¤€). ë¹„ê³ /Notesë§Œìœ¼ë¡œë„ êµ°ì§‘í™”í•˜ë ¤ë©´ ì›ë³¸ ì‹œíŠ¸ì˜ ë¹„ê³ ì—´ì„ í™œìš©í•˜ì‹­ì‹œì˜¤.")
                st.stop()

        # 4) ë¹„ê³ /Notes ë³‘í•©
        with step_status("ë¹„ê³ /Notes ë³‘í•©"):
            df_issue = enrich_with_column_comments(xls, test_sheets_selected[0], df_issue)
            diag_dump("ë³‘í•© ê²°ê³¼ ìƒ˜í”Œ", df_issue.head(10))

        # 5) ìŠ¤íŽ™ ë³‘í•©
        df_final = df_issue.copy()
        match_rate = 0.0
        if spec_sheets_selected:
            with step_status("ìŠ¤íŽ™ ë³‘í•©"):
                try:
                    # ìŠ¤íŽ™ í—¤ë” ìžë™íƒì§€ + í‘œì¤€í™”
                    def _norm_for_header(s: str) -> str:
                        s = unicodedata.normalize("NFKC", str(s))
                        s = re.sub(r"[\s\-\_/()\[\]{}:+Â·âˆ™â€¢]", "", s)
                        return s.lower().strip()

                    def find_header_row_for_spec(xls, sheet, max_scan_rows=12):
                        df_probe = pd.read_excel(xls, sheet_name=sheet, header=None, engine="openpyxl")
                        header_row_idx = 0
                        header_candidates = [r"^model$", r"^device$", r"^ì œí’ˆëª…$", r"^ì œí’ˆ$", r"^ëª¨ë¸ëª…$", r"^ëª¨ë¸$"]
                        for r in range(min(max_scan_rows, len(df_probe))):
                            row_vals = df_probe.iloc[r].astype(str).fillna("")
                            norm_vals = [_norm_for_header(v) for v in row_vals]
                            for v in norm_vals:
                                if any(re.search(pat, v) for pat in header_candidates):
                                    header_row_idx = r; break
                            if header_row_idx: break
                        return header_row_idx

                    def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:
                        original_cols = list(df.columns)
                        norm_cols = [_norm_for_header(c) for c in original_cols]
                        col_map = {}
                        synonyms = {
                            r"^(model|device|ì œí’ˆëª…|ì œí’ˆ|ëª¨ë¸ëª…|ëª¨ë¸)$": "Model",
                            r"^(maker|manufacturer|brand|oem|ì œì¡°ì‚¬|ë²¤ë”)$": "ì œì¡°ì‚¬",
                            r"^(gpu|ê·¸ëž˜í”½|ê·¸ëž˜í”½ì¹©|ê·¸ëž˜í”½ìŠ¤|ê·¸ëž˜í”½í”„ë¡œì„¸ì„œ)$": "GPU",
                            r"^(chipset|soc|ap|cpu)$": "Chipset",
                            r"^(ram|ë©”ëª¨ë¦¬)$": "RAM",
                            r"^(os|osversion|android|ios|íŽŒì›¨ì–´|ì†Œí”„íŠ¸ì›¨ì–´ë²„ì „)$": "OS",
                            r"^(rank|rating|ratinggrade|ë“±ê¸‰)$": "Rank",
                        }
                        for norm_name, orig_name in zip(norm_cols, original_cols):
                            mapped = None
                            for pat, std_name in synonyms.items():
                                if re.search(pat, norm_name):
                                    mapped = std_name; break
                            col_map[orig_name] = mapped or orig_name
                        return df.rename(columns=col_map)

                    def detect_model_col(df: pd.DataFrame):
                        if "Model" in df.columns:
                            return "Model"
                        for c in df.columns:
                            n = _norm_for_header(c)
                            if re.search(r"^(model|device|ì œí’ˆëª…|ì œí’ˆ|ëª¨ë¸ëª…|ëª¨ë¸)$", n):
                                return c
                        return None

                    def load_std_spec_df(xls, sheet):
                        hdr = find_header_row_for_spec(xls, sheet)
                        df = pd.read_excel(xls, sheet_name=sheet, header=hdr, engine="openpyxl")
                        df = standardize_columns(df)
                        model_col = detect_model_col(df)
                        if model_col is None:
                            raise ValueError(f"'{sheet}'ì—ì„œ ëª¨ë¸ ì»¬ëŸ¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì»¬ëŸ¼: {list(df.columns)}")
                        df["model_norm"] = df[model_col].apply(normalize_model_name_strict)
                        cols_keep = ["model_norm"]
                        for c in ["GPU", "ì œì¡°ì‚¬", "Chipset", "RAM", "OS", "Rank", "Model"]:
                            if c in df.columns: cols_keep.append(c)
                        return df[cols_keep]

                    spec_frames = [load_std_spec_df(xls, s) for s in spec_sheets_selected]
                    df_spec_all = pd.concat(spec_frames, ignore_index=True)
                    df_spec_all = df_spec_all.drop_duplicates(subset=["model_norm"], keep="first")

                    df_final["model_norm"] = df_final["Device(Model)"].apply(normalize_model_name_strict)
                    df_final = pd.merge(df_final, df_spec_all, on="model_norm", how="left")

                    # ì ‘ë¯¸ì‚¬ ì •ë¦¬
                    merge_cols = ["GPU", "ì œì¡°ì‚¬", "Chipset", "RAM", "OS", "Rank", "Model"]
                    for col in merge_cols:
                        cx, cy = f"{col}_x", f"{col}_y"
                        if cx in df_final.columns and cy in df_final.columns:
                            df_final[col] = df_final[cx].where(df_final[cx].notna(), df_final[cy])
                            df_final.drop(columns=[cx, cy], inplace=True)
                        elif cx in df_final.columns:
                            df_final.rename(columns={cx: col}, inplace=True)
                        elif cy in df_final.columns:
                            df_final.rename(columns={cy: col}, inplace=True)

                    if "GPU" in df_final.columns:
                        matched = int(df_final["GPU"].notna().sum())
                        match_rate = round(matched / len(df_final) * 100, 1)
                        st.success(f"ìŠ¤íŽ™ ë§¤ì¹­ ê²°ê³¼: {matched} / {len(df_final)} ê±´ ({match_rate}%)")
                except Exception as e:
                    st.error(f"ìŠ¤íŽ™ ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}")

        # 6) ìžê°€ì§„ë‹¨
        with step_status("ëª¨ë“ˆ ìžê°€ì§„ë‹¨"):
            diag = self_check(df_final)
            diag_dump("self_check ê²°ê³¼", diag)
            if not diag["row_ok"]:
                st.error("âŒ ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ. ì¤‘ë‹¨.")
                st.stop()

        # 7) ì½”ë©˜íŠ¸ ì •ê·œí™” ë° Feature íƒœê¹… (df_final ìƒì„± ì´í›„)
        with step_status("ì½”ë©˜íŠ¸ ì •ê·œí™” / íƒœê¹…"):
            def _jamo_norm(s: str) -> str:
                if s is None: return ""
                t = unicodedata.normalize("NFKC", str(s))
                t = re.sub(r"[^0-9a-zA-Zê°€-íž£\s\-_+/.:]", " ", t)
                t = re.sub(r"\s+", " ", t).strip().lower()
                return t

            ISSUE_TAG_PATTERNS = [
                ("punch_hole",   r"(íŽ€ì¹˜í™€|punch[\s\-]?hole|hole[-\s]?camera)"),
                ("notch",        r"(ë…¸ì¹˜|notch)"),
                ("rotation",     r"(íšŒì „|ê°€ë¡œì „í™˜|ì„¸ë¡œì „í™˜|landscape|portrait|rotate)"),
                ("aspect_ratio", r"(í™”ë©´ë¹„|ë¹„ìœ¨|aspect\s?ratio)"),
                ("resolution",   r"(í•´ìƒë„|resolution)"),
                ("cutout",       r"(ì»·ì•„ì›ƒ|cutout)"),
                ("install",      r"(ì„¤ì¹˜\s?ë¶ˆê°€|ì„¤ì¹˜ì˜¤ë¥˜|install\s?fail|íŒ¨í‚¤ì§€\s?ì˜¤ë¥˜|apk\s?ì„¤ì¹˜)"),
                ("permission",   r"(ê¶Œí•œ|permission)"),
                ("login",        r"(ë¡œê·¸ì¸|login|oauth|ì¸ì¦|auth)"),
                ("storage",      r"(ì €ìž¥ê³µê°„|storage|sd\s?card|ê¶Œí•œ\s?ê±°ë¶€)"),
                ("input_lag",    r"(ìž…ë ¥\s?ì§€ì—°|ì§€ì—°\s?ìž…ë ¥|í„°ì¹˜\s?ì§€ì—°|ui\s?ì§€ì—°|input\s?lag|ui\s?lag)"),
                ("keyboard",     r"(í‚¤ë³´ë“œ|ime|keyboard)"),
                ("ui_scaling",   r"(ui\s?ìŠ¤ì¼€ì¼|í™•ëŒ€|ì¶•ì†Œ|dpi|density)"),
                ("render_artifact", r"(ì•„í‹°íŒ©íŠ¸|ê¹¨ì§|ìž”ìƒ|í…Œì–´ë§|ê¸€ë¦¬ì¹˜|artifact|glitch|tearing)"),
                ("black_screen", r"(ê²€ì€\s?í™”ë©´|black\s?screen)"),
                ("white_screen", r"(í•˜ì–€\s?í™”ë©´|white\s?screen)"),
                ("crash",        r"(í¬ëž˜ì‹œ|fatal exception|ê°•ì œì¢…ë£Œ|crash)"),
                ("network",      r"(ë„¤íŠ¸ì›Œí¬|network|ssl|handshake|timeout|unknownhost)"),
                ("audio",        r"(ì†Œë¦¬|ì˜¤ë””ì˜¤|audio|ë¬´ìŒ|ë³¼ë¥¨)"),
                ("camera",       r"(ì¹´ë©”ë¼|camera)"),
                ("thermal",      r"(ì¨ë©€|ë°œì—´|thermal|throttl)"),
                ("fps",          r"(í”„ë ˆìž„|fps)"),
            ]
            def tag_issue_comment(comment: str) -> list:
                s = _jamo_norm(comment)
                tags = []
                for tag, pat in ISSUE_TAG_PATTERNS:
                    if re.search(pat, s, re.I):
                        tags.append(tag)
                # ì¤‘ë³µ ì œê±°
                return list(dict.fromkeys(tags))

            if "comment_text" not in df_final.columns:
                df_final["comment_text"] = ""
            df_final["comment_norm"] = df_final["comment_text"].fillna("").astype(str).apply(_jamo_norm)
            df_final["issue_tags"]   = df_final["comment_text"].fillna("").astype(str).apply(tag_issue_comment)
            diag_dump("íƒœê¹… ìƒ˜í”Œ", df_final[["Device(Model)","GPU","Chipset","OS","comment_text","issue_tags"]].head(15))

        # 8) GPU/Chipset êµ°ì§‘ + Feature êµ°ì§‘
        with step_status("êµ°ì§‘(Cluster) í†µê³„ ì‚°ì¶œ"):
            def _cluster_counts(df, col, topn=15):
                if col not in df.columns:
                    return pd.DataFrame(columns=[col, "count"])
                vc = df[col].fillna("(ë¯¸ê¸°ìž¬)").astype(str).str.strip().value_counts().head(topn)
                return vc.reset_index().rename(columns={"index": col, 0: "count"})
            # GPU ì´ë¦„ ë³´ì •(ê³„ì—´ í†µí•© ì˜ˆ)
            if "GPU" in df_final.columns:
                df_final["GPU"] = (
                    df_final["GPU"].astype(str)
                    .str.replace(r"\bPower\s*VR\b", "PowerVR", regex=True)
                    .str.replace(r"\bIMG\s+GE", "PowerVR GE", regex=True)
                    .str.replace(r"\bGE(\d+)\b", r"PowerVR GE\1", regex=True)
                )
            cluster_gpu = _cluster_counts(df_final, "GPU")
            cluster_chip = _cluster_counts(df_final, "Chipset")
            clusters = {
                "by_gpu": cluster_gpu.to_dict(orient="records"),
                "by_chipset": cluster_chip.to_dict(orient="records"),
            }
            # Feature êµ°ì§‘(íƒœê·¸ ê¸°ë°˜)
            feat_rows = []
            for idx, r in df_final.iterrows():
                for t in (r.get("issue_tags") or []):
                    feat_rows.append({
                        "tag": t,
                        "row_idx": idx,
                        "device": str(r.get("Device(Model)", "")),
                        "gpu": str(r.get("GPU", "")),
                        "chipset": str(r.get("Chipset", "")),
                        "os": str(r.get("OS","")),
                        "comment": str(r.get("comment_text",""))
                    })
            feat_df = pd.DataFrame(feat_rows)
            clusters_feature_detailed = []
            by_issue_tag = []
            if not feat_df.empty:
                g = (feat_df.groupby("tag")
                            .agg(count=("row_idx","size"),
                                 repr_models=("device", lambda s: list(pd.Series(s).dropna().unique())[:3]),
                                 evidence_rows=("row_idx", list))
                            .sort_values("count", ascending=False)
                            .reset_index())
                by_issue_tag = g[["tag","count"]].rename(columns={"tag":"value"}).to_dict(orient="records")
                def _row_evidence(r):
                    return {
                        "row_idx": int(getattr(r, "name", -1)),
                        "device": str(r.get("Device(Model)", "")),
                        "os": str(r.get("OS", "")),
                        "comment": str(r.get("comment_text", ""))[:180]
                    }
                for _, row in g.iterrows():
                    ev = []
                    for ridx in row["evidence_rows"][:6]:
                        rr = df_final.loc[ridx]
                        ev.append(_row_evidence(rr))
                    clusters_feature_detailed.append({
                        "feature_tag": row["tag"],
                        "pattern": row["tag"],
                        "count": int(row["count"]),
                        "repr_models": row["repr_models"],
                        "evidence_rows": ev,
                        "singleton": (int(row["count"]) == 1)
                    })
            diag_dump("GPU/Chipset êµ°ì§‘", clusters)
            diag_dump("Feature êµ°ì§‘ ìš”ì•½", by_issue_tag)
            diag_dump("Feature êµ°ì§‘ ìƒì„¸(ì¼ë¶€)", clusters_feature_detailed[:3])

        # 9) í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ + í† í° ì˜ˆì‚° ì¡°ì •
        metrics = {
            "total_fail_issues": len(df_final),
            "clusters": clusters,
            "by_issue_tag": by_issue_tag,
            "clusters_feature_detailed": clusters_feature_detailed,
            "log_hypotheses": log_hypotheses
        }
        deltas, evidence_links = {}, []

        def _rough_token_count(t: str) -> int:
            return max(1, int(len(t) / 2.5))
        def estimate_tokens(msgs: list) -> int:
            try:
                import tiktoken
                enc = tiktoken.get_encoding("cl100k_base")
                return sum(len(enc.encode(m.get("content",""))) for m in msgs)
            except Exception:
                return sum(_rough_token_count(m.get("content","")) for m in msgs)
        def fit_prompt(build_user, base_kwargs, model_budget=30000, reserve_output=6000):
            max_rows_list = [800, 600, 400, 300, 200, 100]
            df = base_kwargs["sample_issues"]
            for mr in max_rows_list:
                kwargs = dict(base_kwargs)
                kwargs["sample_issues"] = df.head(mr)
                sp = build_system_prompt()
                up = build_user(**kwargs)
                used = estimate_tokens([{"content": sp},{"content": up}])
                if used + reserve_output < model_budget:
                    return sp, up, {"prompt_tokens_est": used, "max_rows": mr}
            return sp, up, {"warn": "budget_exceeded"}

        base_kwargs = {
            "project": "UNKNOWN_PROJECT",
            "version": "UNKNOWN_VERSION",
            "metrics": metrics,
            "deltas": deltas,
            "evidence_links": evidence_links,
            "sample_issues": df_final,
            "max_rows": 500
        }
        with step_status("í† í° ì˜ˆì‚° ì¡°ì •"):
            sp, up, diag_budget = fit_prompt(build_user_prompt, base_kwargs)
            diag_dump("í† í° ì§„ë‹¨", diag_budget)

        # 10) OpenAI í˜¸
Â  Â  Â  Â  with st.spinner("GPTê°€ ë¦¬í¬íŠ¸ë¥¼ ìž‘ì„± ì¤‘ìž…ë‹ˆë‹¤... (429 ì˜¤ë¥˜ ì‹œ ìžë™ ìž¬ì‹œë„)"):
Â  Â  Â  Â  Â  Â  max_retries = 3
Â  Â  Â  Â  Â  Â  wait_time_seconds = 20 # TPM í•œë„ëŠ” 1ë¶„ì„ ê¸°ë‹¤ë ¤ì•¼ í•  ìˆ˜ ìžˆìœ¼ë¯€ë¡œ, ì´ˆê¸° ëŒ€ê¸° ì‹œê°„ì„ ë„‰ë„‰í•˜ê²Œ ì„¤ì •
Â  Â  Â  Â  Â  Â  last_error = None
Â  Â  Â  Â  Â  Â  result = None
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  for attempt in range(max_retries):
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  resp = client.chat.completions.create(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model="gpt-4o",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  temperature=0.1,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  top_p=0.9,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  messages=[{"role":"system","content":sp},{"role":"user","content":up}],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  response_format={"type": "json_object"} # JSON ëª¨ë“œ ê°•ì œ (ì£¼ì„ ë°˜ì˜)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raw = resp.choices[0].message.content
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result = parse_llm_json(raw)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  result["metrics"] = metricsÂ  # êµ°ì§‘/íƒœê·¸ ê·¼ê±° ë³´ì¡´
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  diag_dump("LLM ì›ë¬¸(ìš”ì•½)", raw[:4000])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  last_error = None # ì„±ê³µ ì‹œ ì˜¤ë¥˜ ê¸°ë¡ ì´ˆê¸°í™”
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break # ì„±ê³µ ì‹œ ìž¬ì‹œë„ ë£¨í”„ íƒˆì¶œ
Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  last_error = e
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  error_message = str(e).lower()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # 429 (Rate Limit) ì˜¤ë¥˜ ê°ì§€
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if "rate_limit_exceeded" in error_message or "429" in error_message:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if attempt < max_retries - 1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"â³ RATE LIMIT (429) ê°ì§€ (ì‹œë„ {attempt + 1}/{max_retries}). {wait_time_seconds}ì´ˆ í›„ ìž¬ì‹œë„í•©ë‹ˆë‹¤.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  time.sleep(wait_time_seconds)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wait_time_seconds *= 2 # ëŒ€ê¸° ì‹œê°„ 2ë°° ì¦ê°€ (Exponential Backoff)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ RATE LIMIT (429) ì˜¤ë¥˜. ìž¬ì‹œë„({max_retries}íšŒ) ëª¨ë‘ ì‹¤íŒ¨.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # 429ê°€ ì•„ë‹Œ ë‹¤ë¥¸ ì˜¤ë¥˜ (e.g., 400 Bad Request ë“±)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ OpenAI í˜¸ì¶œ ì¤‘ ë³µêµ¬ ë¶ˆê°€ëŠ¥í•œ ì˜¤ë¥˜ ë°œìƒ: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.stop()

Â  Â  Â  Â  Â  Â  # ìµœì¢…ì ìœ¼ë¡œ resultê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ë‹¤ë©´ ì¤‘ë‹¨
Â  Â  Â  Â  Â  Â  if result is None:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ OpenAI í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {last_error}")
Â  Â  Â  Â  Â  Â  Â  Â  st.stop()        

        # 11) ì—‘ì…€ ë¦¬í¬íŠ¸ ìƒì„±
        try:
            output = "QA_Report.xlsx"
            write_excel_report(result, df_final, output)
            st.success("âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            with open(output, "rb") as f:
                st.download_button("ðŸ“Š Excel ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ", f.read(), file_name=output)
        except Exception as e:
            st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")


