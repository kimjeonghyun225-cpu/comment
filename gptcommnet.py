# -*- coding: utf-8 -*-
# comment1.py â€” í”„ë¡œì íŠ¸ëª… ìž…ë ¥ ì—†ì´, ì„¸ì…˜ ì´ˆê¸°í™” ë²„íŠ¼ìœ¼ë¡œ ëŸ°â†’ë¦¬ì…‹â†’ìž¬ë¶„ì„ í”Œë¡œìš°
# Fail ë¦¬ìŠ¤íŠ¸ â†’ ìŠ¤íŽ™ ë³‘í•© â†’ ì½”ë©˜íŠ¸/ìŠ¤íŽ™ ì •ê·œí™” â†’ êµ°ì§‘í™” â†’ GPT ìš”ì•½ â†’ Excel ë¦¬í¬íŠ¸

import os
import re
import io
import zipfile
import unicodedata
import time
from contextlib import contextmanager

import pandas as pd
import openpyxl
import streamlit as st
from dotenv import load_dotenv
from openai import OpenAI

from qa_patch_module import (
    find_test_sheet_candidates,
    enrich_with_column_comments,
    build_system_prompt, build_user_prompt,
    parse_llm_json, write_excel_report,
    self_check,
    load_std_spec_df,                 # âœ… ìŠ¤íŽ™ ë³‘í•© ìœ í‹¸
    extract_comments_as_dataframe     # âœ… Fail+ì½”ë©˜íŠ¸(ë©”íƒ€ í¬í•¨) ì¶”ì¶œ
)

# =========================
# í™˜ê²½ì„¤ì •
# =========================
load_dotenv()
# ìš°ì„ ìˆœìœ„: st.secrets > .env > os.environ
api_key = st.secrets.get("OPENAI_API_KEY", os.getenv("OPENAI_API_KEY", ""))
if not api_key:
    st.error("OpenAI API í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤. st.secrets ë˜ëŠ” .envì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    st.stop()

client = OpenAI(api_key=api_key)

st.set_page_config(page_title="QA ê²°ê³¼ ìžë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°", layout="wide")
st.title(":bar_chart: QA ê²°ê³¼ ìžë™ ì½”ë©˜íŠ¸ ìƒì„±ê¸°")

# ì„¸ì…˜ ì´ˆê¸°í™” ë²„íŠ¼ë§Œ ì‚¬ìš© (í”„ë¡œì íŠ¸ëª… ìž…ë ¥ UI ì—†ìŒ)
if st.button("ðŸ”„ ì„¸ì…˜ ì´ˆê¸°í™”"):
    st.session_state.clear()
    st.experimental_rerun()

# =========================
# ê³µí†µ ìœ í‹¸
# =========================
@contextmanager
def step_status(title: str):
    with st.status(title, expanded=False) as status:
        t0 = time.time()
        try:
            yield status
            status.update(label=f"{title} - ì™„ë£Œ ({time.time()-t0:.2f}s)", state="complete", expanded=False)
        except Exception as e:
            status.update(label=f"{title} - ì‹¤íŒ¨: {e}", state="error", expanded=True)
            raise

def diag_dump(label: str, obj):
    with st.expander(f"ðŸ”Ž ì§„ë‹¨ ë³´ê¸°: {label}", expanded=False):
        st.write(obj)

def _norm(s: str) -> str:
    s = unicodedata.normalize("NFKC", str(s))
    s = re.sub(r"[\s\-\_/()\[\]{}:+Â·âˆ™â€¢]", "", s)
    return s.lower().strip()

def normalize_model_name_strict(s):
    if pd.isna(s): return ""
    s = str(s)
    s = re.sub(r"\(.*?\)", "", s)
    s = re.sub(r"\b(64|128|256|512)\s*gb\b", "", s, flags=re.I)
    s = re.sub(r"\b(black|white|blue|red|green|gold|silver|ê³¨ë“œ|ë¸”ëž™|í™”ì´íŠ¸|ì‹¤ë²„)\b", "", s, flags=re.I)
    s = re.sub(r"[\s\-_]+", "", s)
    return s.lower().strip()

# =========================
# Log ë¶„ì„: ìš”ì•½ + ê·¼ë³¸ ì›ì¸ ì¶”ì • (í˜„ìž¬ ì—…ë¡œë“œ ë¹„í™œì„±)
# =========================
def load_and_summarize_logcat_files(files):
    patterns = {
        "crash": re.compile(r"\bFATAL EXCEPTION\b|\bAbort message:\b|\bbacktrace\b", re.I),
        "anr": re.compile(r"\bANR in\b|\bApplication Not Responding\b", re.I),
        "gl_err": re.compile(r"(E/libEGL|E/GLConsumer|OpenGLRenderer|Adreno|Mali)", re.I),
        "thermal": re.compile(r"(thermal|ThermalEngine|throttl)", re.I),
        "net": re.compile(r"(SocketTimeout|UnknownHost|SSLHandshake|Network is unreachable)", re.I),
        "fps": re.compile(r"\bFPS[:=]\s*\d+", re.I),
    }
    total_counts = {k: 0 for k in patterns.keys()}
    file_count = 0

    def _consume_text(txt: str):
        for k, p in patterns.items():
            total_counts[k] += len(p.findall(txt))

    for f in files:
        name = f.name.lower()
        try:
            if name.endswith(".zip"):
                with zipfile.ZipFile(io.BytesIO(f.read())) as zf:
                    for info in zf.infolist():
                        if info.is_dir(): continue
                        if not info.filename.lower().endswith((".txt", ".log")): continue
                        with zf.open(info) as zfh:
                            data = zfh.read()
                            txt = data.decode("utf-8", errors="ignore")
                            _consume_text(txt)
                            file_count += 1
            else:
                data = f.read()
                txt = data.decode("utf-8", errors="ignore")
                _consume_text(txt)
                file_count += 1
        except Exception:
            continue

    parts = [f"{k}:{v}" for k, v in total_counts.items()]
    return {"log_summary": f"files={file_count}; " + ", ".join(parts)}

def _parse_log_summary(summary: str) -> dict:
    out = {"files": 0, "crash": 0, "anr": 0, "gl_err": 0, "thermal": 0, "net": 0, "fps": 0}
    if not summary:
        return out
    try:
        parts = [p.strip() for p in summary.split(";")]
        if parts and parts[0].startswith("files="):
            out["files"] = int(parts[0].split("=",1)[1])
        tail = parts[1] if len(parts) > 1 else ""
        for kv in tail.split(","):
            if ":" in kv:
                k, v = kv.split(":", 1)
                if k.strip() in out:
                    out[k.strip()] = int(v.strip())
    except Exception:
        pass
    return out

def infer_root_causes_from_logs(summary: str) -> list:
    c = _parse_log_summary(summary)
    hyps = []
    if c.get("gl_err", 0) >= 3 or (c.get("fps", 0) >= 10 and c.get("gl_err", 0) >= 1):
        hyps.append({"signal": "gl_err/fps", "hypothesis": "GPU ë“œë¼ì´ë²„/ë Œë”ë§ ë³‘ëª© ê°€ëŠ¥", "evidence": f"gl_err={c.get('gl_err',0)}, fps={c.get('fps',0)}"})
    if c.get("crash", 0) >= 2:
        hyps.append({"signal": "crash", "hypothesis": "ë„¤ì´í‹°ë¸Œ í¬ëž˜ì‹œ(ë©”ëª¨ë¦¬/ë„í¬ì¸í„°) ê°€ëŠ¥", "evidence": f"crash={c.get('crash',0)}"})
    if c.get("anr", 0) >= 1:
        hyps.append({"signal": "anr", "hypothesis": "ë©”ì¸ìŠ¤ë ˆë“œ ë¸”ë¡œí‚¹/IO ì§€ì—°ìœ¼ë¡œ ì¸í•œ ANR ê°€ëŠ¥", "evidence": f"anr={c.get('anr',0)}"})
    if c.get("thermal", 0) >= 1:
        hyps.append({"signal": "thermal", "hypothesis": "ì¨ë©€ ìŠ¤ë¡œí‹€ë§ìœ¼ë¡œ ì¸í•œ í´ëŸ­ ì €í•˜", "evidence": f"thermal={c.get('thermal',0)}"})
    if c.get("net", 0) >= 2:
        hyps.append({"signal": "net", "hypothesis": "ë„¤íŠ¸ì›Œí¬ ì§€ì—°/SSL ì˜¤ë¥˜ë¡œ ì¸í•œ UX ì €í•˜ ê°€ëŠ¥", "evidence": f"net={c.get('net',0)}"})
    return hyps

# =========================
# UI: íŒŒì¼ ì—…ë¡œë“œ
# =========================
uploaded_file = st.file_uploader("ì›ë³¸ QA ì—‘ì…€ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["xlsx"])
log_files = None   # í•„ìš” ì‹œ í™œì„±í™”
st.caption("â€» Logcat ë¶„ì„ì€ í˜„ìž¬ ë¹„í™œì„±í™” ìƒíƒœìž…ë‹ˆë‹¤. (ì„¸ì…˜ ì´ˆê¸°í™” í›„ ìƒˆ íŒŒì¼ë¡œ ìž¬ë¶„ì„í•˜ì„¸ìš”)")

if uploaded_file:
    with step_status("ì—‘ì…€ ë¡œë“œ"):
        xls = pd.ExcelFile(uploaded_file, engine="openpyxl")
        diag_dump("ì‹œíŠ¸ ëª©ë¡", xls.sheet_names)

    # 1) í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ìžë™ê°ì§€
    with step_status("í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ìžë™ê°ì§€"):
        test_candidates = find_test_sheet_candidates(xls)
        diag_dump("ê°ì§€ëœ í›„ë³´ ì‹œíŠ¸", test_candidates)

    st.subheader("1. í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ì„ íƒ")
    test_sheets_selected = st.multiselect(
        "ìžë™ ê°ì§€ëœ í…ŒìŠ¤íŠ¸ ì‹œíŠ¸ ì¤‘ ë¶„ì„ ëŒ€ìƒ ì„ íƒ",
        options=test_candidates,
        default=test_candidates[:2]
    )
    if not test_sheets_selected:
        st.error("âŒ ìµœì†Œ 1ê°œ ì´ìƒ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤.")
        st.stop()

    # 2) ìŠ¤íŽ™ ì‹œíŠ¸ ì„ íƒ
    st.subheader("2. ìŠ¤íŽ™ ì‹œíŠ¸ ì„ íƒ (ë””ë°”ì´ìŠ¤ ë¦¬ìŠ¤íŠ¸)")
    default_spec = [s for s in ["AOS_Device_List", "iOS_Device_List"] if s in xls.sheet_names]
    spec_sheets_selected = st.multiselect(
        "ìŠ¤íŽ™(Chipset, GPU, OS, Rank ë“±) í¬í•¨ ì‹œíŠ¸ ì„ íƒ",
        options=xls.sheet_names,
        default=default_spec
    )
    st.markdown("---")

    if st.button("ë¶„ì„ ë° ë¦¬í¬íŠ¸ ìƒì„±", type="primary"):
        # ì‹¤í–‰ë³„ ìƒíƒœë³€ìˆ˜ ì´ˆê¸°í™” (ë‹¤ë¥¸ í”„ë¡œì íŠ¸ ê²°ê³¼ ì„žìž„ ë°©ì§€)
        log_summary = {}
        log_hypotheses = []
        clusters = {}
        evidence_links = []

        # 3) Fail + ì…€ ì½”ë©˜íŠ¸ ì¶”ì¶œ
        with step_status("Fail + ì…€ ì½”ë©˜íŠ¸ ì¶”ì¶œ"):
            wb = openpyxl.load_workbook(uploaded_file, data_only=True)
            df_issue = extract_comments_as_dataframe(wb, test_sheets_selected)
            diag_dump("Fail ì¶”ì¶œ ìƒ˜í”Œ(ìµœëŒ€ 10)", df_issue.head(10) if df_issue is not None else None)

        if df_issue is None or df_issue.empty:
            st.warning("âŒ Fail+ì½”ë©˜íŠ¸ í•­ëª©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            st.stop()

        # 4) ë¹„ê³ /Notes ë³‘í•©
        with step_status("ë¹„ê³ /Notes ë³‘í•©"):
            df_issue = enrich_with_column_comments(xls, test_sheets_selected[0], df_issue)
            diag_dump("ë³‘í•© ê²°ê³¼ ìƒ˜í”Œ", df_issue.head(10))

        # 5) ìŠ¤íŽ™ ì‹œíŠ¸ ë³‘í•©
        df_final = df_issue.copy()
        match_rate = 0.0
        if spec_sheets_selected:
            with step_status("ìŠ¤íŽ™ ë³‘í•©"):
                try:
                    spec_frames = [load_std_spec_df(xls, s) for s in spec_sheets_selected]
                    df_spec_all = pd.concat(spec_frames, ignore_index=True).drop_duplicates(subset=["model_norm"], keep="first")

                    df_final["model_norm"] = df_final["Device(Model)"].apply(normalize_model_name_strict)
                    df_final = pd.merge(df_final, df_spec_all, on="model_norm", how="left")

                    for col in ["GPU","ì œì¡°ì‚¬","Chipset","RAM","OS","Rank","Model"]:
                        cx, cy = f"{col}_x", f"{col}_y"
                        if cx in df_final.columns and cy in df_final.columns:
                            df_final[col] = df_final[cx].where(df_final[cx].notna(), df_final[cy])
                            df_final.drop(columns=[cx, cy], inplace=True)
                        elif cx in df_final.columns:
                            df_final.rename(columns={cx: col}, inplace=True)
                        elif cy in df_final.columns:
                            df_final.rename(columns={cy: col}, inplace=True)

                    if "GPU" in df_final.columns:
                        matched = int(df_final["GPU"].notna().sum())
                        match_rate = round(matched / len(df_final) * 100, 1)
                        st.success(f"ìŠ¤íŽ™ ë§¤ì¹­ ê²°ê³¼: {matched} / {len(df_final)} ê±´ ({match_rate}%)")
                except Exception as e:
                    st.error(f"ìŠ¤íŽ™ ë³‘í•© ì¤‘ ì˜¤ë¥˜: {e}")
        else:
            df_final = df_issue.copy()

        # 6) ëª¨ë“ˆ ìžê°€ì§„ë‹¨
        with step_status("ëª¨ë“ˆ ìžê°€ì§„ë‹¨"):
            diag = self_check(df_final)
            diag_dump("self_check ê²°ê³¼", diag)
            if not diag["row_ok"]:
                st.error("âŒ ìœ íš¨í•œ ë°ì´í„° ì—†ìŒ. ì¤‘ë‹¨.")
                st.stop()

        # 7) ì½”ë©˜íŠ¸ ì •ë¦¬ + GPU ì •ê·œí™”/ê³„ì—´ ë¶„ë¥˜ + Chipset ê¸°ë°˜ ë³´ê°•
        def clean_comment_text(s: str) -> str:
            if pd.isna(s): return ""
            t = str(s)
            t = re.sub(r"https?://go\.microsoft\.com/.*", " ", t)
            t = re.sub(r"Excelì—ì„œ ì´ ìŠ¤ë ˆë“œ ëŒ“ê¸€ì„.*?ìžì„¸í•œ ì •ë³´.*?:", " ", t)
            t = re.sub(r"\s+", " ", t).strip(" -:|,.;\n\t")
            for pat, rep in [
                (r"í”„ë ˆìž„\s*ë“œëž|í”„ë ˆìž„ë“œëž|í”„ë ˆìž„\s*ì €í•˜|í”„ë ˆìž„\s*í•˜ë½", "í”„ë ˆìž„ ë“œëž"),
                (r"ë ‰|ëž™|ë²„ë²…|ë²„ë²…ìž„|ëŠê¹€|ì§€ì—°", "ìž…ë ¥ ì§€ì—°"),
                (r"ë°œì—´|ê³¼ì—´", "ë°œì—´"),
                (r"í¬ëž˜ì‹œ|ê°•ì œì¢…ë£Œ|íŠ•ê¹€", "í¬ëž˜ì‹œ"),
                (r"í…ìŠ¤ì²˜\s*ê¹¨ì§|ê·¸ëž˜í”½\s*ê¹¨ì§|ë Œë”ë§\s*ì˜¤ë¥˜", "ê·¸ëž˜í”½ ê¹¨ì§"),
                (r"í™”ë©´\s*íšŒì „\s*ë¶ˆê°€|íšŒì „\s*ì•ˆë¨", "í™”ë©´ íšŒì „ ë¬¸ì œ"),
                (r"ANR|ì‘ë‹µì—†ìŒ", "ANR"),
            ]:
                t = re.sub(pat, rep, t, flags=re.I)
            return t

        df_final["comment_text_norm"] = df_final.get("comment_text", "").astype(str).apply(clean_comment_text)

        def normalize_gpu_name(s: str) -> str:
            if pd.isna(s) or not str(s).strip(): return ""
            x = str(s).strip().replace("â€“","-").replace("â€”","-").replace("_"," ")
            x = re.sub(r"\s+"," ", x)
            x = re.sub(r"\bPower\s*VR\b", "PowerVR", x, flags=re.I)
            x = re.sub(r"\bIMG\s+GE", "PowerVR GE", x, flags=re.I)
            x = re.sub(r"\bIMG\s+GT", "PowerVR GT", x, flags=re.I)
            x = re.sub(r"\bGE(\d+)\b", r"PowerVR GE\1", x, flags=re.I)
            x = re.sub(r"\bGT(\d+)\b", r"PowerVR GT\1", x, flags=re.I)
            x = re.sub(r"\bAdreno\s*-?\s*(\d+)", r"Adreno \1", x, flags=re.I)
            x = re.sub(r"\bMali[\s\-]*G\s*(\d+)\s*MP?\s*(\d+)\b", r"Mali-G\1 MP\2", x, flags=re.I)
            x = re.sub(r"\bMali[\s\-]*G\s*(\d+)\b", r"Mali-G\1", x, flags=re.I)
            x = re.sub(r"\bMali[\s\-]*T\s*(\d+)\s*MP?\s*(\d+)\b", r"Mali-T\1 MP\2", x, flags=re.I)
            x = re.sub(r"\bMali[\s\-]*T\s*(\d+)\b", r"Mali-T\1", x, flags=re.I)
            x = re.sub(r"\bApple\s*(GPU)?\s*\(?(\d+)\s*[- ]?core\)?", r"Apple GPU \2-core", x, flags=re.I)
            x = re.sub(r"\bVivante\s*(GC|GT)\s*(\d+)", r"Vivante \1\2", x, flags=re.I)
            x = re.sub(r"\bTegra\s*(K1|X1|X2)\b", r"Tegra \1", x, flags=re.I)
            return x

        def classify_gpu_family(x: str) -> str:
            y = (x or "").lower()
            if "adreno" in y: return "Adreno"
            if "mali" in y: return "Mali"
            if "powervr" in y or "img ge" in y or "img gt" in y: return "PowerVR"
            if "apple gpu" in y: return "Apple"
            if "vivante" in y: return "Vivante"
            if "tegra" in y or "nvidia" in y: return "Tegra"
            return "Other" if y else ""

        def infer_gpu_from_chipset(s: str) -> str:
            t = ("" if pd.isna(s) else str(s)).lower()
            if not t: return ""
            if "snapdragon" in t or "qualcomm" in t: return "Adreno (inferred)"
            if "mediatek" in t or "dimensity" in t or "helio" in t: return "Mali (inferred)"
            if "exynos" in t: return "Mali (inferred)"
            if "kirin" in t or "hisilicon" in t: return "Mali (inferred)"
            if re.search(r"\bapple\s*a\d+\b", t): return "Apple GPU (inferred)"
            if "unisoc" in t or "spreadtrum" in t: return "Mali (inferred)"
            return ""

        df_final["GPU"] = df_final.get("GPU","").astype(str).apply(normalize_gpu_name)
        miss = df_final["GPU"].eq("") | df_final["GPU"].isna()
        if "Chipset" in df_final.columns and miss.any():
            df_final.loc[miss, "GPU"] = df_final.loc[miss, "Chipset"].apply(infer_gpu_from_chipset)
        df_final["GPU_Family"] = df_final["GPU"].apply(classify_gpu_family)

        # 8) êµ°ì§‘(Cluster) í†µê³„ + ìƒì„¸(ê³„ì—´Ã—ì¦ìƒ) ìƒì„±
        with step_status("êµ°ì§‘(Cluster) í†µê³„ ì‚°ì¶œ"):
            def _cluster_counts(df, col, topn=20):
                if col not in df.columns:
                    st.info(f"êµ°ì§‘ ìŠ¤í‚µ: '{col}' ì—†ìŒ"); return pd.DataFrame(columns=[col,"count"])
                nn = int(df[col].replace("", pd.NA).notna().sum())
                if nn == 0:
                    st.info(f"êµ°ì§‘ ìŠ¤í‚µ: '{col}' ëª¨ë‘ ê²°ì¸¡"); return pd.DataFrame(columns=[col,"count"])
                vc = (df[col].fillna("(ë¯¸ê¸°ìž¬)").astype(str).str.strip()
                      .replace("", "(ë¯¸ê¸°ìž¬)").value_counts().head(topn))
                return vc.reset_index().rename(columns={"index": col, 0: "count"})

            cluster_gpu_family = _cluster_counts(df_final, "GPU_Family")
            cluster_gpu_model  = _cluster_counts(df_final, "GPU")
            cluster_chip       = _cluster_counts(df_final, "Chipset")

            def build_signature(s: str) -> str:
                t = ("" if pd.isna(s) else str(s)).lower()
                keep = []
                for kw in ["í”„ë ˆìž„ ë“œëž","ê·¸ëž˜í”½ ê¹¨ì§","ìž…ë ¥ ì§€ì—°","í¬ëž˜ì‹œ","ë°œì—´","í™”ë©´ íšŒì „ ë¬¸ì œ","anr","ë„¤íŠ¸ì›Œí¬","ì‚¬ìš´ë“œ","ë¡œë”© ì§€ì—°","ë©”ëª¨ë¦¬"]:
                    if kw in t: keep.append(kw)
                return " | ".join(sorted(set(keep))) or t[:40]

            df_final["issue_signature"] = df_final["comment_text_norm"].apply(build_signature)

            def top_models(s, n=3):
                return [str(x) for x in pd.Series(s).dropna().astype(str).head(n)]

            grp = (df_final
                   .groupby(["GPU_Family","issue_signature"], dropna=False)
                   .agg(count=("issue_signature","size"),
                        repr_models=("Device(Model)", lambda s: top_models(s, 3)),
                        evidence_rows=("comment_text_norm", lambda s: [str(x) for x in pd.Series(s).dropna().astype(str).head(3)]))
                   .reset_index()
                   .sort_values("count", ascending=False))

            clusters = {
                "by_gpu_family": cluster_gpu_family.to_dict(orient="records"),
                "by_gpu":        cluster_gpu_model.to_dict(orient="records"),
                "by_chipset":    cluster_chip.to_dict(orient="records"),
                "detailed": [
                    {
                        "dimension": "GPU_Family",
                        "value": r["GPU_Family"] or "",
                        "signature": r["issue_signature"] or "",
                        "count": int(r["count"]),
                        "repr_models": r["repr_models"],
                        "evidence_rows": r["evidence_rows"]
                    }
                    for _, r in grp.iterrows() if r["count"] >= 2
                ]
            }
            diag_dump("êµ°ì§‘ í†µê³„/ìƒì„¸", clusters)

        # 9) í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (í”„ë¡œì íŠ¸/ë²„ì „ì€ ë¹ˆê°’ìœ¼ë¡œ ì „ë‹¬)
        metrics = {
            "total_fail_issues": len(df_final),
            "by_gpu_family": clusters["by_gpu_family"],
            "by_gpu": clusters["by_gpu"],
            "by_chipset": clusters["by_chipset"],
            "clusters_detailed": clusters["detailed"],
            "log_hypotheses": []  # log_files ë¹„í™œì„± ìƒíƒœ
        }
        deltas, evidence_links = {}, []

        base_kwargs = {
            "project": "",                 # âœ… í”„ë¡œì íŠ¸ëª… ë¯¸ì‚¬ìš©
            "version": "",                 # âœ… ë²„ì „ ë¯¸ì‚¬ìš©
            "metrics": metrics,
            "deltas": deltas,
            "evidence_links": evidence_links,
            "sample_issues": df_final,
            "max_rows": 500
        }

        # 10) í† í° ì˜ˆì‚° ìžë™ ì¡°ì •
        def _rough_token_count(t: str) -> int:
            return max(1, int(len(t) / 2.5))
        def estimate_tokens(msgs: list) -> int:
            try:
                import tiktoken
                enc = tiktoken.get_encoding("cl100k_base")
                return sum(len(enc.encode(m.get("content",""))) for m in msgs)
            except Exception:
                return sum(_rough_token_count(m.get("content","")) for m in msgs)
        def fit_prompt(build_user, base_kwargs, model_budget=120000, reserve_output=6000):
            max_rows_list = [800, 600, 400, 300, 200, 100]
            df = base_kwargs["sample_issues"]
            for mr in max_rows_list:
                kwargs = dict(base_kwargs)
                kwargs["sample_issues"] = df.head(mr)
                sp = build_system_prompt()
                up = build_user(**kwargs)
                used = estimate_tokens([{"content": sp},{"content": up}])
                if used + reserve_output < model_budget:
                    return sp, up, {"prompt_tokens_est": used, "max_rows": mr}
            return sp, up, {"warn": "budget_exceeded"}

        with step_status("í† í° ì˜ˆì‚° ì¡°ì •"):
            sp, up, diag_budget = fit_prompt(build_user_prompt, base_kwargs)
            diag_dump("í† í° ì§„ë‹¨", diag_budget)

        # 11) OpenAI í˜¸ì¶œ
        with st.spinner("GPTê°€ ë¦¬í¬íŠ¸ë¥¼ ìž‘ì„± ì¤‘ìž…ë‹ˆë‹¤..."):
            try:
                resp = client.chat.completions.create(
                    model="gpt-4o",
                    temperature=0.1,
                    top_p=0.9,
                    messages=[{"role":"system","content":sp},{"role":"user","content":up}],
                )
                raw = resp.choices[0].message.content
                result = parse_llm_json(raw)
                result["metrics"] = metrics
                diag_dump("LLM ì›ë¬¸(ìš”ì•½)", raw[:4000])
            except Exception as e:
                st.error(f"OpenAI í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                st.stop()

        # 12) ì—‘ì…€ ë¦¬í¬íŠ¸ ìƒì„±
        try:
            output = "QA_Report.xlsx"
            write_excel_report(result, df_final, output)
            st.success("âœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
            with open(output, "rb") as f:
                st.download_button("ðŸ“Š Excel ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ", f.read(), file_name=output)
        except Exception as e:
            st.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
